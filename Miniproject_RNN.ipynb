{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Miniproject_RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "B3aE0lPcFnOO",
        "colab_type": "code",
        "outputId": "f67a95df-cc04-4117-b95a-407bd1bd2bbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Mount google drive for loading and saving files \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "m6Ngos6-aJeh",
        "colab_type": "code",
        "outputId": "b25b9989-6a82-4efe-95a6-eb31b5ba04fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Setting up tensorboard\n",
        "LOG_DIR = 'drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_log'\n",
        "get_ipython().system_raw('tensorboard --logdir {} --host 127.0.0.1 --port 6007 &'.format(LOG_DIR))\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://d3efa040.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6gZ5rMml9yX9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Importing all necessary libraries and defining constants\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib import layers\n",
        "from tensorflow.contrib import rnn\n",
        "import tensorboard\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import glob\n",
        "import sys\n",
        "import re\n",
        "tf.set_random_seed(0)\n",
        "ALPHASIZE = 93\n",
        "INTERNALSIZE = 512\n",
        "NLAYERS = 3\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "__jjOknBH1hq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Provided by Martin Görner\n",
        "def convert_from_alphabet(a):\n",
        "    \"\"\"Encode a character\n",
        "    :param a: one character\n",
        "    :return: the encoded value\n",
        "    \"\"\"\n",
        "    if a == 9:\n",
        "        return 1\n",
        "    if a == 10:\n",
        "        return 127 - 30  # LF\n",
        "    elif 32 <= a <= 126:\n",
        "        return a - 30\n",
        "    else:\n",
        "        return 0  # unknown\n",
        "\n",
        "\n",
        "# encoded values:\n",
        "# unknown = 0\n",
        "# tab = 1\n",
        "# space = 2\n",
        "# all chars from 32 to 126 = c-30\n",
        "# LF mapped to 127-30\n",
        "def convert_to_alphabet(c, avoid_tab_and_lf=False):\n",
        "    \"\"\"Decode a code point\n",
        "    :param c: code point\n",
        "    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n",
        "    :return: decoded character\n",
        "    \"\"\"\n",
        "    if c == 1:\n",
        "        return 32 if avoid_tab_and_lf else 9  # space instead of TAB\n",
        "    if c == 127 - 30:\n",
        "        return 92 if avoid_tab_and_lf else 10  # \\ instead of LF\n",
        "    if 32 <= c + 30 <= 126:\n",
        "        return c + 30\n",
        "    else:\n",
        "        return 0  # unknown\n",
        "\n",
        "\n",
        "def encode_text(s):\n",
        "    \"\"\"Encode a string.\n",
        "    :param s: a text string\n",
        "    :return: encoded list of code points\n",
        "    \"\"\"\n",
        "    s = s.replace('ü','ue').replace('ä','ae').replace('ö','oe')\n",
        "    return list(map(lambda a: convert_from_alphabet(ord(a)), s))\n",
        "\n",
        "\n",
        "def decode_to_text(c, avoid_tab_and_lf=False):\n",
        "    \"\"\"Decode an encoded string.\n",
        "    :param c: encoded list of code points\n",
        "    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return \"\".join(map(lambda a: chr(convert_to_alphabet(a, avoid_tab_and_lf)), c))\n",
        "\n",
        "\n",
        "def sample_from_probabilities(probabilities, topn=ALPHASIZE):\n",
        "    \"\"\"Roll the dice to produce a random integer in the [0..ALPHASIZE] range,\n",
        "    according to the provided probabilities. If topn is specified, only the\n",
        "    topn highest probabilities are taken into account.\n",
        "    :param probabilities: a list of size ALPHASIZE with individual probabilities\n",
        "    :param topn: the number of highest probabilities to consider. Defaults to all of them.\n",
        "    :return: a random integer\n",
        "    \"\"\"\n",
        "    p = np.squeeze(probabilities)\n",
        "    p[np.argsort(p)[:-topn]] = 0\n",
        "    p = p / np.sum(p)\n",
        "    return np.random.choice(ALPHASIZE, 1, p=p)[0]\n",
        "\n",
        "\n",
        "def rnn_minibatch_sequencer(raw_data, batch_size, sequence_size, nb_epochs):\n",
        "    \"\"\"\n",
        "    Divides the data into batches of sequences so that all the sequences in one batch\n",
        "    continue in the next batch. This is a generator that will keep returning batches\n",
        "    until the input data has been seen nb_epochs times. Sequences are continued even\n",
        "    between epochs, apart from one, the one corresponding to the end of raw_data.\n",
        "    The remainder at the end of raw_data that does not fit in an full batch is ignored.\n",
        "    :param raw_data: the training text\n",
        "    :param batch_size: the size of a training minibatch\n",
        "    :param sequence_size: the unroll size of the RNN\n",
        "    :param nb_epochs: number of epochs to train on\n",
        "    :return:\n",
        "        x: one batch of training sequences\n",
        "        y: on batch of target sequences, i.e. training sequences shifted by 1\n",
        "        epoch: the current epoch number (starting at 0)\n",
        "    \"\"\"\n",
        "    data = np.array(raw_data)\n",
        "    data_len = data.shape[0]\n",
        "    # using (data_len-1) because we must provide for the sequence shifted by 1 too\n",
        "    nb_batches = (data_len - 1) // (batch_size * sequence_size)\n",
        "    assert nb_batches > 0, \"Not enough data, even for a single batch. Try using a smaller batch_size.\"\n",
        "    rounded_data_len = nb_batches * batch_size * sequence_size\n",
        "    xdata = np.reshape(data[0:rounded_data_len], [batch_size, nb_batches * sequence_size])\n",
        "    ydata = np.reshape(data[1:rounded_data_len + 1], [batch_size, nb_batches * sequence_size])\n",
        "\n",
        "    for epoch in range(nb_epochs):\n",
        "        for batch in range(nb_batches):\n",
        "            x = xdata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
        "            y = ydata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
        "            x = np.roll(x, -epoch, axis=0)  # to continue the text from epoch to epoch (do not reset rnn state!)\n",
        "            y = np.roll(y, -epoch, axis=0)\n",
        "            yield x, y, epoch\n",
        "\n",
        "\n",
        "def find_book(index, bookranges):\n",
        "    return next(\n",
        "        book[\"name\"] for book in bookranges if (book[\"start\"] <= index < book[\"end\"]))\n",
        "\n",
        "\n",
        "def find_book_index(index, bookranges):\n",
        "    return next(\n",
        "        i for i, book in enumerate(bookranges) if (book[\"start\"] <= index < book[\"end\"]))\n",
        "\n",
        "\n",
        "def print_learning_learned_comparison(X, Y, losses, bookranges, batch_loss, batch_accuracy, epoch_size, index, epoch):\n",
        "    \"\"\"Display utility for printing learning statistics\"\"\"\n",
        "    print()\n",
        "    # epoch_size in number of batches\n",
        "    batch_size = X.shape[0]  # batch_size in number of sequences\n",
        "    sequence_len = X.shape[1]  # sequence_len in number of characters\n",
        "    start_index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n",
        "    for k in range(batch_size):\n",
        "      if k % 50 == 0:\n",
        "        index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n",
        "        decx = decode_to_text(X[k], avoid_tab_and_lf=True)\n",
        "        decy = decode_to_text(Y[k], avoid_tab_and_lf=True)\n",
        "        bookname = find_book(index_in_epoch, bookranges)\n",
        "        formatted_bookname = \"{: <10.40}\".format(bookname)  # min 10 and max 40 chars\n",
        "        epoch_string = \"{:4d}\".format(index) + \" (epoch {}) \".format(epoch)\n",
        "        loss_string = \"loss: {:.5f}\".format(losses[k])\n",
        "        print_string = epoch_string + formatted_bookname + \" │ {} │ {} │ {}\"\n",
        "        print(print_string.format(decx, decy, loss_string))\n",
        "        index += sequence_len\n",
        "    # box formatting characters:\n",
        "    # │ \\u2502\n",
        "    # ─ \\u2500\n",
        "    # └ \\u2514\n",
        "    # ┘ \\u2518\n",
        "    # ┴ \\u2534\n",
        "    # ┌ \\u250C\n",
        "    # ┐ \\u2510\n",
        "    format_string = \"└{:─^\" + str(len(epoch_string)) + \"}\"\n",
        "    format_string += \"{:─^\" + str(len(formatted_bookname)) + \"}\"\n",
        "    format_string += \"┴{:─^\" + str(len(decx) + 2) + \"}\"\n",
        "    format_string += \"┴{:─^\" + str(len(decy) + 2) + \"}\"\n",
        "    format_string += \"┴{:─^\" + str(len(loss_string)) + \"}┘\"\n",
        "    footer = format_string.format('INDEX', 'BOOK NAME', 'TRAINING SEQUENCE', 'PREDICTED SEQUENCE', 'LOSS')\n",
        "    print(footer)\n",
        "    # print statistics\n",
        "    batch_index = start_index_in_epoch // (batch_size * sequence_len)\n",
        "    batch_string = \"batch {}/{} in epoch {},\".format(batch_index, epoch_size, epoch)\n",
        "    stats = \"{: <28} batch loss: {:.5f}, batch accuracy: {:.5f}\".format(batch_string, batch_loss, batch_accuracy)\n",
        "    print()\n",
        "    print(\"TRAINING STATS: {}\".format(stats))\n",
        "\n",
        "\n",
        "class Progress:\n",
        "    \"\"\"Text mode progress bar.\n",
        "    Usage:\n",
        "            p = Progress(30)\n",
        "            p.step()\n",
        "            p.step()\n",
        "            p.step(start=True) # to restart form 0%\n",
        "    The progress bar displays a new header at each restart.\"\"\"\n",
        "    def __init__(self, maxi, size=100, msg=\"\"):\n",
        "        \"\"\"\n",
        "        :param maxi: the number of steps required to reach 100%\n",
        "        :param size: the number of characters taken on the screen by the progress bar\n",
        "        :param msg: the message displayed in the header of the progress bat\n",
        "        \"\"\"\n",
        "        self.maxi = maxi\n",
        "        self.p = self.__start_progress(maxi)()  # () to get the iterator from the generator\n",
        "        self.header_printed = False\n",
        "        self.msg = msg\n",
        "        self.size = size\n",
        "\n",
        "    def step(self, reset=False):\n",
        "        if reset:\n",
        "            self.__init__(self.maxi, self.size, self.msg)\n",
        "        if not self.header_printed:\n",
        "            self.__print_header()\n",
        "        next(self.p)\n",
        "\n",
        "    def __print_header(self):\n",
        "        print()\n",
        "        format_string = \"0%{: ^\" + str(self.size - 6) + \"}100%\"\n",
        "        print(format_string.format(self.msg))\n",
        "        self.header_printed = True\n",
        "\n",
        "    def __start_progress(self, maxi):\n",
        "        def print_progress():\n",
        "            # Bresenham's algorithm. Yields the number of dots printed.\n",
        "            # This will always print 100 dots in max invocations.\n",
        "            dx = maxi\n",
        "            dy = self.size\n",
        "            d = dy - dx\n",
        "            for x in range(maxi):\n",
        "                k = 0\n",
        "                while d >= 0:\n",
        "                    print('=', end=\"\", flush=True)\n",
        "                    k += 1\n",
        "                    d -= dx\n",
        "                d += dy\n",
        "                yield k\n",
        "\n",
        "        return print_progress\n",
        "\n",
        "def clean_text_from_emojies(shaketext):\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "                       u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                       u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                       u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                       u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                       u\"\\U00002702-\\U000027B0\"\n",
        "                       u\"\\U000024C2-\\U0001F251\"\n",
        "                       u\"\\U0001f926-\\U0001f937\"\n",
        "                       u\"\\u200d\"\n",
        "                       u\"\\u2640-\\u2642\" \n",
        "                       \"]+\", flags=re.UNICODE)\n",
        "  return emoji_pattern.sub('',shaketext.read())\n",
        "\n",
        "def read_data_files(directory,validation=True):\n",
        "    \"\"\"Read data files according to the specified glob pattern\n",
        "    Optionnaly set aside the last file as validation data.\n",
        "    No validation data is returned if there are 5 files or less.\n",
        "    :param directory: for example \"data/*.txt\"\n",
        "    :param validation: if True (default), sets the last file aside as validation data\n",
        "    :return: training data, validation data, list of loaded file names with ranges\n",
        "     If validation is\n",
        "    \"\"\"\n",
        "    codetext = []\n",
        "    bookranges = []\n",
        "    shakelist = glob.glob(directory, recursive=True)\n",
        "    print(shakelist)\n",
        "    p = Progress(len(shakelist))\n",
        "    for shakefile in shakelist:\n",
        "        p.step()\n",
        "        try:\n",
        "          shaketext = open(shakefile, \"r\")\n",
        "          start = len(codetext)\n",
        "          #codetext.extend(encode_text(shaketext.read()))\n",
        "          codetext.extend(encode_text(clean_text_from_emojies(shaketext)))\n",
        "          end = len(codetext)\n",
        "          bookranges.append({\"start\": start, \"end\": end, \"name\": shakefile.rsplit(\"/\", 1)[-1]})\n",
        "          shaketext.close()\n",
        "        except:\n",
        "          continue\n",
        "    if len(bookranges) == 0:\n",
        "        sys.exit(\"No training data has been found. Aborting.\")\n",
        "\n",
        "    # For validation, use roughly 90K of text,\n",
        "    # but no more than 10% of the entire text\n",
        "    # and no more than 1 book in 5 => no validation at all for 5 files or fewer.\n",
        "\n",
        "    # 10% of the text is how many files ?\n",
        "    total_len = len(codetext)\n",
        "    validation_len = 0\n",
        "    nb_books1 = 0\n",
        "    for book in reversed(bookranges):\n",
        "        validation_len += book[\"end\"]-book[\"start\"]\n",
        "        nb_books1 += 1\n",
        "        if validation_len > total_len // 10:\n",
        "            break\n",
        "\n",
        "    # 90K of text is how many books ?\n",
        "    validation_len = 0\n",
        "    nb_books2 = 0\n",
        "    for book in reversed(bookranges):\n",
        "        validation_len += book[\"end\"]-book[\"start\"]\n",
        "        nb_books2 += 1\n",
        "        if validation_len > 90*1024:\n",
        "            break\n",
        "\n",
        "    # 20% of the books is how many books ?\n",
        "    nb_books3 = len(bookranges) // 5\n",
        "\n",
        "    # pick the smallest\n",
        "    nb_books = min(nb_books1, nb_books2, nb_books3)\n",
        "    \n",
        "    if nb_books == 0:\n",
        "      cutoff = len(codetext) - int(len(codetext)/5)\n",
        "    else:\n",
        "      cutoff = bookranges[-nb_books][\"start\"]\n",
        "    valitext = codetext[cutoff:]\n",
        "    codetext = codetext[:cutoff]\n",
        "    return codetext, valitext, bookranges\n",
        "\n",
        "\n",
        "def print_data_stats(datalen, valilen, epoch_size):\n",
        "    datalen_mb = datalen/1024.0/1024.0\n",
        "    valilen_kb = valilen/1024.0\n",
        "    print(\"\\nTraining text size is {:.2f}MB with {:.2f}KB set aside for validation.\".format(datalen_mb, valilen_kb)\n",
        "          + \" There will be {} batches per epoch\".format(epoch_size))\n",
        "\n",
        "\n",
        "def print_validation_header(validation_start, bookranges):\n",
        "    bookindex = find_book_index(validation_start, bookranges)\n",
        "    books = ''\n",
        "    for i in range(bookindex, len(bookranges)):\n",
        "        books += bookranges[i][\"name\"]\n",
        "        if i < len(bookranges)-1:\n",
        "            books += \", \"\n",
        "    print(\"{: <60}\".format(\"\\nValidating on \" + books), flush=True)\n",
        "\n",
        "\n",
        "def print_validation_stats(loss, accuracy):\n",
        "    print(\"\\nVALIDATION STATS:                                  loss: {:.5f},       accuracy: {:.5f}\".format(loss,\n",
        "                                                                                                           accuracy))\n",
        "\n",
        "\n",
        "def print_text_generation_header():\n",
        "    print()\n",
        "    print(\"┌{:─^111}┐\".format('Generating random text from learned state'))\n",
        "\n",
        "\n",
        "def print_text_generation_footer():\n",
        "    print()\n",
        "    print(\"└{:─^111}┘\".format('End of generation'))\n",
        "\n",
        "\n",
        "def frequency_limiter(n, multiple=1, modulo=0):\n",
        "    def limit(i):\n",
        "        return i % (multiple * n) == modulo*multiple\n",
        "    return limit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xZsJJbcfOR_k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training RNN\n",
        "def run_training_procedure(modelname, file_dir ,model_dir,log_dir ,seq_len=50):\n",
        "  tf.reset_default_graph()\n",
        "  SEQLEN = seq_len\n",
        "  BATCHSIZE = 200\n",
        "  learning_rate = 0.001  # fixed learning rate\n",
        "  dropout_pkeep = 0.8    # some dropout\n",
        "\n",
        "  # load data, either shakespeare, or the Python source of Tensorflow itself\n",
        "  shakedir = file_dir+'/*.txt'\n",
        "  codetext, valitext, bookranges = read_data_files(shakedir,validation=True)\n",
        "  # display some stats on the data\n",
        "  epoch_size = len(codetext) // (BATCHSIZE * SEQLEN)\n",
        "  print_data_stats(len(codetext), len(valitext), epoch_size)\n",
        "\n",
        "  #\n",
        "  # the model (see FAQ in README.md)\n",
        "  #\n",
        "  lr = tf.placeholder(tf.float32, name='lr')  # learning rate\n",
        "  pkeep = tf.placeholder(tf.float32, name='pkeep')  # dropout parameter\n",
        "  batchsize = tf.placeholder(tf.int32, name='batchsize')\n",
        "\n",
        "  # inputs\n",
        "  X = tf.placeholder(tf.uint8, [None, None], name='X')    # [ BATCHSIZE, SEQLEN ]\n",
        "  Xo = tf.one_hot(X, ALPHASIZE, 1.0, 0.0)                 # [ BATCHSIZE, SEQLEN, ALPHASIZE ]\n",
        "  # expected outputs = same sequence shifted by 1 since we are trying to predict the next character\n",
        "  Y_ = tf.placeholder(tf.uint8, [None, None], name='Y_')  # [ BATCHSIZE, SEQLEN ]\n",
        "  Yo_ = tf.one_hot(Y_, ALPHASIZE, 1.0, 0.0)               # [ BATCHSIZE, SEQLEN, ALPHASIZE ]\n",
        "  # input state\n",
        "  Hin = tf.placeholder(tf.float32, [None, INTERNALSIZE*NLAYERS], name='Hin')  # [ BATCHSIZE, INTERNALSIZE * NLAYERS]\n",
        "\n",
        "  # using a NLAYERS=3 layers of GRU cells, unrolled SEQLEN=30 times\n",
        "  # dynamic_rnn infers SEQLEN from the size of the inputs Xo\n",
        "\n",
        "  # How to properly apply dropout in RNNs: see README.md\n",
        "  cells = [rnn.GRUCell(INTERNALSIZE) for _ in range(NLAYERS)]\n",
        "  # \"naive dropout\" implementation\n",
        "  dropcells = [rnn.DropoutWrapper(cell,input_keep_prob=pkeep) for cell in cells]\n",
        "  multicell = rnn.MultiRNNCell(dropcells, state_is_tuple=False)\n",
        "  multicell = rnn.DropoutWrapper(multicell, output_keep_prob=pkeep)  # dropout for the softmax layer\n",
        "\n",
        "  Yr, H = tf.nn.dynamic_rnn(multicell, Xo, dtype=tf.float32, initial_state=Hin)\n",
        "  # Yr: [ BATCHSIZE, SEQLEN, INTERNALSIZE ]\n",
        "  # H:  [ BATCHSIZE, INTERNALSIZE*NLAYERS ] # this is the last state in the sequence\n",
        "\n",
        "  H = tf.identity(H, name='H')  # just to give it a name\n",
        "\n",
        "  # Softmax layer implementation:\n",
        "  # Flatten the first two dimension of the output [ BATCHSIZE, SEQLEN, ALPHASIZE ] => [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
        "  # then apply softmax readout layer. This way, the weights and biases are shared across unrolled time steps.\n",
        "  # From the readout point of view, a value coming from a sequence time step or a minibatch item is the same thing.\n",
        "\n",
        "  Yflat = tf.reshape(Yr, [-1, INTERNALSIZE])    # [ BATCHSIZE x SEQLEN, INTERNALSIZE ]\n",
        "  Ylogits = layers.linear(Yflat, ALPHASIZE)     # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
        "  Yflat_ = tf.reshape(Yo_, [-1, ALPHASIZE])     # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
        "  loss = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Yflat_)  # [ BATCHSIZE x SEQLEN ]\n",
        "  loss = tf.reshape(loss, [batchsize, -1])      # [ BATCHSIZE, SEQLEN ]\n",
        "  Yo = tf.nn.softmax(Ylogits, name='Yo')        # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
        "  Y = tf.argmax(Yo, 1)                          # [ BATCHSIZE x SEQLEN ]\n",
        "  Y = tf.reshape(Y, [batchsize, -1], name=\"Y\")  # [ BATCHSIZE, SEQLEN ]\n",
        "  train_step = tf.train.AdamOptimizer(lr).minimize(loss)\n",
        "\n",
        "  # stats for display\n",
        "  seqloss = tf.reduce_mean(loss, 1)\n",
        "  batchloss = tf.reduce_mean(seqloss)\n",
        "  accuracy = tf.reduce_mean(tf.cast(tf.equal(Y_, tf.cast(Y, tf.uint8)), tf.float32))\n",
        "  loss_summary = tf.summary.scalar(\"batch_loss\", batchloss)\n",
        "  acc_summary = tf.summary.scalar(\"batch_accuracy\", accuracy)\n",
        "  summaries = tf.summary.merge([loss_summary, acc_summary])\n",
        "\n",
        "  # Init Tensorboard stuff. This will save Tensorboard information into a different\n",
        "  # folder at each run named 'log/<timestamp>/'. Two sets of data are saved so that\n",
        "  # you can compare training and validation curves visually in Tensorboard.\n",
        "  timestamp = str(math.trunc(time.time()))\n",
        "  summary_writer = tf.summary.FileWriter(log_dir+\"/\" + timestamp + \"-training\")\n",
        "  validation_writer = tf.summary.FileWriter(log_dir+\"/\" + timestamp + \"-validation\")\n",
        "\n",
        "  # Init for saving models. They will be saved into a directory named 'checkpoints'.\n",
        "  # Only the last checkpoint is kept.\n",
        "  saver = tf.train.Saver(max_to_keep=10)\n",
        "\n",
        "  # for display: init the progress bar\n",
        "  DISPLAY_FREQ = 50\n",
        "  _50_BATCHES = DISPLAY_FREQ * BATCHSIZE * SEQLEN\n",
        "  progress = Progress(DISPLAY_FREQ, size=111+2, msg=\"Training on next \"+str(DISPLAY_FREQ)+\" batches\")\n",
        "\n",
        "  # init\n",
        "  istate = np.zeros([BATCHSIZE, INTERNALSIZE*NLAYERS])  # initial zero input state\n",
        "  init = tf.global_variables_initializer()\n",
        "  sess = tf.Session()\n",
        "  sess.run(init)\n",
        "  step = 0\n",
        "\n",
        "  # training loop\n",
        "  for x, y_, epoch in rnn_minibatch_sequencer(codetext, BATCHSIZE, SEQLEN, nb_epochs=10):\n",
        "\n",
        "      # train on one minibatch\n",
        "      feed_dict = {X: x, Y_: y_, Hin: istate, lr: learning_rate, pkeep: dropout_pkeep, batchsize: BATCHSIZE}\n",
        "      _, y, ostate = sess.run([train_step, Y, H], feed_dict=feed_dict)\n",
        "\n",
        "      # log training data for Tensorboard display a mini-batch of sequences (every 50 batches)\n",
        "      if step  % _50_BATCHES == 0:\n",
        "          feed_dict = {X: x, Y_: y_, Hin: istate, pkeep: 1.0, batchsize: BATCHSIZE}  # no dropout for validation\n",
        "          y, l, bl, acc, smm = sess.run([Y, seqloss, batchloss, accuracy, summaries], feed_dict=feed_dict)\n",
        "          print_learning_learned_comparison(x, y, l, bookranges, bl, acc, epoch_size, step, epoch)\n",
        "          summary_writer.add_summary(smm, step)\n",
        "\n",
        "      # run a validation step every 50 batches\n",
        "      # The validation text should be a single sequence but that's too slow (1s per 1024 chars!),\n",
        "      # so we cut it up and batch the pieces (slightly inaccurate)\n",
        "      # tested: validating with 5K sequences instead of 1K is only slightly more accurate, but a lot slower.\n",
        "      if step % _50_BATCHES == 0 and len(valitext) > 0:\n",
        "          VALI_SEQLEN = 1*1024  # Sequence length for validation. State will be wrong at the start of each sequence.\n",
        "          bsize = len(valitext) // VALI_SEQLEN\n",
        "          print_validation_header(len(codetext), bookranges)\n",
        "          vali_x, vali_y, _ = next(rnn_minibatch_sequencer(valitext, bsize, VALI_SEQLEN, 1))  # all data in 1 batch\n",
        "          vali_nullstate = np.zeros([bsize, INTERNALSIZE*NLAYERS])\n",
        "          feed_dict = {X: vali_x, Y_: vali_y, Hin: vali_nullstate, pkeep: 1.0,  # no dropout for validation\n",
        "                       batchsize: bsize}\n",
        "          ls, acc, smm = sess.run([batchloss, accuracy, summaries], feed_dict=feed_dict)\n",
        "          print_validation_stats(ls, acc)\n",
        "          # save validation data for Tensorboard\n",
        "          validation_writer.add_summary(smm, step)\n",
        "\n",
        "      # display a short text generated with the current weights and biases (every 150 batches)\n",
        "      if step // 6 % _50_BATCHES == 0:\n",
        "          print_text_generation_header()\n",
        "          ry = np.array([[convert_from_alphabet(ord(\"K\"))]])\n",
        "          rh = np.zeros([1, INTERNALSIZE * NLAYERS])\n",
        "          for k in range(100):\n",
        "              ryo, rh = sess.run([Yo, H], feed_dict={X: ry, pkeep: 1.0, Hin: rh, batchsize: 1})\n",
        "              rc = sample_from_probabilities(ryo, topn=10 if epoch <= 1 else 2)\n",
        "              print(chr(convert_to_alphabet(rc)), end=\"\")\n",
        "              ry = np.array([[rc]])\n",
        "          print_text_generation_footer()\n",
        "\n",
        "      # save a checkpoint (every 500 batches)\n",
        "      if step // 10 % _50_BATCHES == 0:\n",
        "          saved_file = saver.save(sess, model_dir + '/rnn_train_' + timestamp, global_step=step)\n",
        "          print(\"Saved file: \" + saved_file)\n",
        "\n",
        "      # display progress bar\n",
        "      progress.step(reset=step % _50_BATCHES == 0)\n",
        "\n",
        "      # loop state around\n",
        "      istate = ostate\n",
        "      step += BATCHSIZE * SEQLEN\n",
        "\n",
        "  return saver.save(sess, model_dir + '/final_rnn_train_' + modelname, global_step=step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x8x10KCfE9yw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Using the generated model to generate text\n",
        "def generate_sample(topn=10,author=''):\n",
        "\n",
        "  # use topn=10 for all but the last one which works with topn=2 for Shakespeare and topn=3 for Python\n",
        "  ncnt = 0\n",
        "  output_string = ''\n",
        "  with tf.Session() as sess:\n",
        "      new_saver = tf.train.import_meta_graph(author+'.meta')\n",
        "      new_saver.restore(sess, author)\n",
        "      x = convert_from_alphabet(ord(\"L\"))\n",
        "      x = np.array([[x]])  # shape [BATCHSIZE, SEQLEN] with BATCHSIZE=1 and SEQLEN=1\n",
        "\n",
        "      # initial values\n",
        "      y = x\n",
        "      h = np.zeros([1, INTERNALSIZE * NLAYERS], dtype=np.float32)  # [ BATCHSIZE, INTERNALSIZE * NLAYERS]\n",
        "      for i in range(10000):\n",
        "          yo, h = sess.run(['Yo:0', 'H:0'], feed_dict={'X:0': y, 'pkeep:0': 1., 'Hin:0': h, 'batchsize:0': 1})\n",
        "\n",
        "          # If sampling is be done from the topn most likely characters, the generated text\n",
        "          # is more credible and more \"english\". If topn is not set, it defaults to the full\n",
        "          # distribution (ALPHASIZE)\n",
        "\n",
        "          # Recommended: topn = 10 for intermediate checkpoints, topn=2 or 3 for fully trained checkpoints\n",
        "\n",
        "          c = sample_from_probabilities(yo, topn=topn)\n",
        "          y = np.array([[c]])  # shape [BATCHSIZE, SEQLEN] with BATCHSIZE=1 and SEQLEN=1\n",
        "          c = chr(convert_to_alphabet(c))\n",
        "          output_string+=c\n",
        "  return output_string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M-BApdJ9532A",
        "colab_type": "code",
        "outputId": "c61a681a-fb94-48d5-af95-f40998aec025",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5460
        }
      },
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "model_path = 'drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_models'\n",
        "file_path='drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_corpus'\n",
        "log_path ='drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_log'\n",
        "model_name = run_training_procedure('chats',model_dir=model_path,file_dir = file_path,log_dir = log_path,seq_len=200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_corpus/chat_papa.txt', 'drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_corpus/chat_rafi.txt', 'drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_corpus/chat_paula.txt', 'drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_corpus/chat_mama.txt', 'drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_corpus/chat_saunsch.txt', 'drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_corpus/chat_vera.txt', 'drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_corpus/sofa_chat.txt', 'drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_corpus/chat_familie.txt', 'drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_corpus/chat_soziologie.txt', 'drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_corpus/chat_felix.txt']\n",
            "\n",
            "0%                                                                                              100%\n",
            "====================================================================================================\n",
            "Training text size is 3.93MB with 596.40KB set aside for validation. There will be 102 batches per epoch\n",
            "\n",
            "   0 (epoch 0) chat_papa.txt │ [09.12.13, 06:16:07] Holzmano: Noii whatsapp nummere\\[09.12.13, 09:02:06] Papa: D.h. Xamer etz da au hijribe?\\[09.12.13, 09:06:23] Holzmano: Ja klar...\\[09.12.13, 09:06:29] Holzmano: :)\\[09.12.13, 11: │                                                                                                                                                                                                          │ loss: 4.31079\n",
            " 200 (epoch 0) chat_papa.txt │ woehnt dasme cha schribe und de ander denns au gseht und antwort git...\\[09.11.17, 23:35:17] Holzmano: Und ohni telefon gaat das ener weniger\\[09.11.17, 23:35:57] Paula Guzman: Ja, das isch war. Han d │                                                                                                                                                                                                          │ loss: 4.31525\n",
            " 400 (epoch 0) chat_papa.txt │  Ned vergesse - morn abig *29.5* isch *FiT-Sitzig mit aschluessender Coworking-Vorprojekt-Sitzig*! Choemed alli zahlriich - sgit einiges zverzelle und zbespreche!\\\\Ort: *Pfarrgasse 1*, Kirchgemeindeha │                                                                                                                                                                                                          │ loss: 4.38399\n",
            " 600 (epoch 0) chat_papa.txt │ s? Wie churz?\\\u0000[25.04.14, 10:14:28] Vera: \u0000Bild weggelassen\\[25.04.14, 10:15:05] Holzmano: Ah dasch guet...\\[25.04.14, 10:15:36] Vera: Findi nid\\[25.04.14, 10:15:42] Vera: Mir gfallts nid soo\\[25.04.1 │                                                                                                                                                                                                          │ loss: 4.29653\n",
            "└─────INDEX───────BOOK NAME──┴────────────────────────────────────────────────────────────────────────────────────────────TRAINING SEQUENCE─────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────PREDICTED SEQUENCE────────────────────────────────────────────────────────────────────────────────────────────┴────LOSS─────┘\n",
            "\n",
            "TRAINING STATS: batch 0/102 in epoch 0,      batch loss: 4.32091, batch accuracy: 0.12890\n",
            "\n",
            "Validating on chat_felix.txt                               \n",
            "\n",
            "VALIDATION STATS:                                  loss: 4.30702,       accuracy: 0.11562\n",
            "\n",
            "┌───────────────────────────────────Generating random text from learned state───────────────────────────────────┐\n",
            "]oeenmdnshr ereer ra daeaed r aade1d1rde1 r1at::e:a1:001aa1 00eea0 o: 0ntaeon et:oe0n 0rno eoes0as0n\n",
            "└───────────────────────────────────────────────End of generation───────────────────────────────────────────────┘\n",
            "Saved file: drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_models/rnn_train_1548172463-0\n",
            "\n",
            "0%                                        Training on next 50 batches                                        100%\n",
            "=================================================================================================================\n",
            "2000000 (epoch 0) chat_vera.txt │ Holzmano: Ich dich au!!\\[14.01.14, 19:40:20] Holzmano: Schlaf guet!\\[14.01.14, 19:40:54] Papa: Ix ga nani... Mus noxli jappe. Du?\\[14.01.14, 19:41:24] Holzmano: Gad etzt!\\[14.01.14, 19:42:18] Papa: Ja │  aaa                    .....111111:::::]]  aaaa                    .....111111:::::]]  aaa                ..                     ....111,11::::::]]  aaaa                 .....111111:::::]]  aaaa      │ loss: 2.73902\n",
            "2000200 (epoch 0) chat_vera.txt │  2 Fraue gschlafe\\[12.11.17, 23:18:07] Paula Guzman: Aso ihr uebertriebet gern\\[12.11.17, 23:18:47] Holzmano: Oder \u0000Ich bin a einere dra\u0000 = Ich han da eini kenneglernt und si isch mega e cooli und ich │                   .....11,, 1:::::]]  aaa                                 e     ....111,11:::::]]  aaaa                                    eee e                       eee eeee            e             │ loss: 2.84673\n",
            "2000400 (epoch 0) chat_vera.txt │  17:09:06] Holzmano: abem 6i esse und abem 7ni igenden GD...du kennsch sicher es paar! Hett luet us de ph, de uni und de eth\\[18.09.18, 17:25:14] Vera: fasch au go essev\\[18.09.18, 17:25:19] Vera: *ga │ 1::::::]]  aaaa                                             .  ee     ee        ee         eeee  e ee         e               .....1111 :::::]]  aaaa                      ....111111:::::]]  aaaa       │ loss: 2.86150\n",
            "2000600 (epoch 0) chat_vera.txt │ 14, 13:59:23] Holzmano: \u0000Audio weggelassen\\\u0000[28.04.14, 13:59:42] Papa: \u0000Bild weggelassen\\[28.04.14, 13:59:48] Papa: Besser\\[28.04.14, 13:59:55] Holzmano: Ja aber es isch ja de familie chat ned de laen │ 11111:::::]]  aaaa                          .....111111:::::]]  aaaa                     .....111111:::::]]  aaaa           ....111111::::::]  aaaa                                                      │ loss: 2.68752\n",
            "└──────INDEX─────────BOOK NAME──┴────────────────────────────────────────────────────────────────────────────────────────────TRAINING SEQUENCE─────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────PREDICTED SEQUENCE────────────────────────────────────────────────────────────────────────────────────────────┴────LOSS─────┘\n",
            "\n",
            "TRAINING STATS: batch 50/102 in epoch 0,     batch loss: 2.84685, batch accuracy: 0.20070\n",
            "\n",
            "Validating on chat_felix.txt                               \n",
            "\n",
            "VALIDATION STATS:                                  loss: 2.83399,       accuracy: 0.18541\n",
            "\n",
            "0%                                        Training on next 50 batches                                        100%\n",
            "=================================================================================================================\n",
            "4000000 (epoch 0) chat_soziologie.txt │ minere kreditcharte han? Und draechnig cheque? Die sett iwo sii!\\\u0000[20.02.14, 11:30:35] Papa: \u0000Audio weggelassen\\[20.02.14, 11:30:49] Papa: Isch guet?\\\u0000[20.02.14, 11:31:33] Papa: \u0000Bild weggelassen\\[20. │ ee    ee           ee   ee  ee        ee   e   e   e   eee eee    00...11,, 1::::::]] Hali  Sf e   eeeee e      00...11,, 1::::::]] Hali: Sfhh eee    [0....11,, 1::::::]] Hali: Sfuie eeeeeeeeee   00.. │ loss: 2.32751\n",
            "4000200 (epoch 0) chat_soziologie.txt │ .11.17, 18:21:54] Paula Guzman: Ohni kuss nur lache\\[14.11.17, 18:22:24] Paula Guzman: Que tal = wie haesche es?\\[14.11.17, 18:25:26] Holzmano: Ja das weisi sogar...aber sooooo es bloeds (neus) schild │ ...11, 1::::::0] Halia oozann:  a   eee  ee  ee     10...11,, 1::::::]] Halia oozann:  aa  e  eeeee ee   e  ee   00...11,  1::::::]] Holzaann:  e ee  ee    eeee   [[ e   ee eeeee eee    ee     eee     │ loss: 2.30922\n",
            "4000400 (epoch 0) chat_soziologie.txt │  gh\u0000rt mir\\[18.01.19, 14:31:25] Holzmano: Ok hansi gnoh\\[28.03.17, 21:20:00] \u0000Christina  Soziologie hat die Gruppe \u0000SoFa \u0000 erstellt.\\[28.03.17, 21:20:00] \u0000Christina  Soziologie hat dich hinzugefuegt.\\ │ iee  eee   10...11,, 1::::::]] Holzaann:    ee    ee    00...11,, 1::::::]] Halia ooo  oooioooooee ee  ee  eee    e eeeeeeee     e  [00...11,, 1::::::]] Halia ooo  oooioooooee ee  ee   ee   ee  e      │ loss: 2.46633\n",
            "4000600 (epoch 0) chat_soziologie.txt │ 22] Felix: \\[09.05.14, 12:07:26] Papa: Erj mojn\\[09.05.14, 12:07:42] Felix: Ja abr weli ziit?\\[09.05.14, 12:09:04] Felix: Am morge frueeh?\\[09.05.14, 12:09:15] Papa: Am morge oeppe 9i fahri app, ga na │ 0] Holia  So .....1,, 1:::::5]  Hala   f  eee   00...11,, 1::::::]] Holia  So e e eee  eee    00...11,, 1::::::]] Holia  Soeeee e ee e    [0....11,, 1::::::]] Hali: Sfaiee e ee  e e  ee    e    eeeee  │ loss: 2.33565\n",
            "└──────INDEX────────────BOOK NAME─────┴────────────────────────────────────────────────────────────────────────────────────────────TRAINING SEQUENCE─────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────PREDICTED SEQUENCE────────────────────────────────────────────────────────────────────────────────────────────┴────LOSS─────┘\n",
            "\n",
            "TRAINING STATS: batch 100/102 in epoch 0,    batch loss: 2.41661, batch accuracy: 0.30692\n",
            "\n",
            "Validating on chat_felix.txt                               \n",
            "\n",
            "VALIDATION STATS:                                  loss: 2.38434,       accuracy: 0.32013\n",
            "\n",
            "0%                                        Training on next 50 batches                                        100%\n",
            "=================================================================================================================\n",
            "6000000 (epoch 1) chat_saunsch.txt │  Waennt a soeis anelaufj bringjmers hei?!\\[21.03.14, 20:15:36] Papa: Farbton: blau moeglixjt wenig birchermueesli-stile... \\[22.03.14, 05:36:40] Holzmano: Haha ok ich probiers...!\\Ou mann und bi ois i │ [e     deae    ie     eeee   eee   ae   [[0.....17, 1:::::1]] Hala: \u0000ae e  e [ee  aa  eee e [ee  eeee h   ee   e   e e [[[[e2.....17, 1:::::1]] Holzmano: Hae  aeeaeh wae e    [[[[[[ [e   de  de dee ie │ loss: 2.44040\n",
            "6000200 (epoch 1) chat_saunsch.txt │  aagschisse uf de zug zga...villicht hettmers gmerkt!\\[15.11.17, 17:55:33] Paula Guzman: Und huet bim tschueues saege.. uiuiui\\[15.11.17, 17:55:40] Holzmano: Haha voll...!\\[15.11.17, 17:56:04] Holzman │ aa   h     ae de deeeaeee...e    h  ae   e   aae  e  [0.....17, 1:::::18] Halaa Suzman: \u0000a  dee  ae eee h      ae  e [[[e      0.....1,, 1:::::1]] Holzmano: Hae  aa ee...[[0.....17, 1:::::13] Holzmano │ loss: 1.95639\n",
            "6000400 (epoch 1) chat_saunsch.txt │  han d dateie vom sofa chat uf telegram duregluegt \\[04.04.17, 10:03:05] Christina  Soziologie: und denkt ich teile all die schoene moment nomal mit eu \\[04.04.17, 10:03:39] Sadhbh Soziologie: hahaha  │ \u0000a  dede     de eeeee eh   ae de   ee eae   eee eeae2.....17, 1:::::18] Haris inn SSoziologie: \u0000a  de  e deh de  e deeedee deh     de e   ae e eeee ee de2.....17, 1:::::13] HoliaanSoziologie: \u0000a     a │ loss: 2.07526\n",
            "6000600 (epoch 1) chat_saunsch.txt │ [26.05.14, 14:26:23] Mama: Gsesch i has dich tankt.\\Laenz bisch am fuulaenzele u bade?\\[26.05.14, 14:27:32] Felix: Gnau duu mit dim (freudsche) augekomplex \\[26.05.14, 14:40:45] Papa: Gaell I hate sal │ 2.....17, 1:::::18] Hala: \u0000aes h dede  de h wee   [[e   e[ee h deedee e   eee deeee   [2.....17, 1:::::1]] Halia: \u0000ae  de  de  de ee[ee    h   [aee e eee  [[2.....17, 1:::::18] Hala: \u0000ae  edede   deee │ loss: 2.21850\n",
            "└──────INDEX──────────BOOK NAME────┴────────────────────────────────────────────────────────────────────────────────────────────TRAINING SEQUENCE─────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────PREDICTED SEQUENCE────────────────────────────────────────────────────────────────────────────────────────────┴────LOSS─────┘\n",
            "\n",
            "TRAINING STATS: batch 48/102 in epoch 1,     batch loss: 2.02758, batch accuracy: 0.41752\n",
            "\n",
            "Validating on chat_felix.txt                               \n",
            "\n",
            "VALIDATION STATS:                                  loss: 2.05272,       accuracy: 0.40098\n",
            "\n",
            "0%                                        Training on next 50 batches                                        100%\n",
            "=================================================================================================================\n",
            "8000000 (epoch 1) chat_soziologie.txt │ assen\\[05.04.14, 18:12:29] Papa: Na?\\\u0000[05.04.14, 18:14:45] Papa: \u0000Bild weggelassen\\[05.04.14, 18:14:52] Papa: Din toeff\\[05.04.14, 18:15:12] Papa: Facetime?\\[05.04.14, 20:55:50] Holzmano: Srry bin am  │ ssen [04.02.17, 18:2::27] Hapa: \u0000as [[08.02.17, 18:2::17] Hapa: \u0000Bild weggelssssn[[04.02.17, 18:2::27] Hapa: \u0000as de   ee24.02.17, 18:2::17] Hapa: \u0000ash      [02.02.17, 18:2::27] Holzmano: Hoe  wic ds d │ loss: 1.26724\n",
            "8000200 (epoch 1) chat_soziologie.txt │ er inere Pause...\\\\In den Ausgang mit oder ohne Alkohol? Diese 10 Situationen bereiten dich auf beides vor --- https://www.watson.ch/!404558668?utm_medium=earned&utm_source=whatsapp&utm_campaign=share │ r ss    sans   ..[[  der s   en  eas si r sie  ssee   e [as   s[.iic         iir      ie h is eeer    iir se  iaeee  [[ieegigssee.hee[[[[....1, 11:eer   e  s er    ee  e hr ees  see  eeehseessee  es s │ loss: 3.38869\n",
            "8000400 (epoch 1) chat_soziologie.txt │ hi\\[04.04.17, 11:33:15] Christina  Soziologie: grindr isch so unterhaltsam\\[04.04.17, 11:33:25] Christina  Soziologie: vo tinder und grindr\\[04.04.17, 11:33:31] Holzmano: ok...ich chume nomal uf mini  │    02.02.17, 18:2::17] Hhristina  Soziologie: haes   ss h dc de    ene  n  04.02.17, 18:2::17] Hhristina  Soziologie: ha de     se  dees    12.02.17, 11:2::27] Holzmano: Hei...hh dhe erse e  seese   d │ loss: 1.44489\n",
            "8000600 (epoch 1) chat_soziologie.txt │ Papa: Tel dr sager 052-212-50-10\\[02.06.14, 09:25:50] Papa: Ly\\[02.06.14, 18:32:49] Papa: Fahre gad app\\[02.06.14, 18:33:43] Felix: \u0000oeoeoeoe\\[02.06.14, 18:33:57] Papa: Kkk\\[02.06.14, 18:34:19] Felix: │ apa: \u0000anise sss r s[ 0..0.  1  [[17.06.17, 18:2::12] Hapa: \u0000a i12.02.17, 19:2::17] Hapa: \u0000as   ses ssee 24.02.17, 11:2::17] Helix: \u0000Bil l lgl 04.02.17, 18:2::17] Hapa: \u0000aie 28.02.17, 18:2::17] Helix:  │ loss: 1.57632\n",
            "└──────INDEX────────────BOOK NAME─────┴────────────────────────────────────────────────────────────────────────────────────────────TRAINING SEQUENCE─────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────PREDICTED SEQUENCE────────────────────────────────────────────────────────────────────────────────────────────┴────LOSS─────┘\n",
            "\n",
            "TRAINING STATS: batch 98/102 in epoch 1,     batch loss: 1.83344, batch accuracy: 0.45022\n",
            "\n",
            "Validating on chat_felix.txt                               \n",
            "\n",
            "VALIDATION STATS:                                  loss: 1.79962,       accuracy: 0.46344\n",
            "\n",
            "0%                                        Training on next 50 batches                                        100%\n",
            "=================================================================================================================\n",
            "10000000 (epoch 2) chat_saunsch.txt │ a: Ha mini vile bildr vom i fon ufde compi synxronisiert... Wo sinds daenn etz?\\[06.05.14, 08:39:16] Papa: Xani feisse?\\\u0000[06.05.14, 08:39:59] Papa: \u0000Audio weggelassen\\[06.05.14, 08:40:02] Holzmano: Ma │ : \u0000aese   do   gee   sor ssdee eee  sh ee se             . [aede    een   dr   [00.02.17, 10:58:38] Hapa: \u0000e   dar     [[00.02.17, 10:08:38] Hapa: \u0000Bid   weggelassen[[00.02.17, 10:38:38] Holzmano: Han │ loss: 1.75127\n",
            "10000200 (epoch 2) chat_saunsch.txt │ 5:41:40] Paula Guzman: Isch mir au ehrlichgseit egal.\\[17.11.17, 15:42:35] Paula Guzman: \\[17.11.17, 15:43:47] Holzmano:  da bini froh!\\[17.11.17, 15:44:59] Paula Guzman: Aso aecht.\\[17.11.17, 15:48:1 │ :44:38] Hapla Guzman: Hc h dac ss dr  e h      dn e  [00.02.17, 10:36:38] Hapla Guzman: H[17.02.17, 10:52:38] Holzmano: HBandee  dee   [00.02.17, 10:36:33] Hapla Guzman: Hn  dn h   [00.02.17, 10:36:38 │ loss: 1.25925\n",
            "10000400 (epoch 2) chat_saunsch.txt │ 17, 21:07:06] Fabienne Soziologie: Demfall doch noed vill zu wiit denkt\\[05.04.17, 21:08:04] Xenia Soziologie: Also ich han das mit em merz-benz au witzig gfonde obwohl i de insider ned kennt han \\[07 │ 7, 17:23:33] Helixnn  Soziologie: Ia  e   se h ao   ao  eseedee  eer   [00.02.17, 10:58:38] Henia Soziologie: \u0000n   dch das ee  ean er der   er  dn dee    aeee   seee  essserse      se  eer   den ee00. │ loss: 1.82495\n",
            "10000600 (epoch 2) chat_saunsch.txt │ tha...\\[06.05.14, 18:25:36] \u0000+91\u000080878\u000029911\u0000: \\[06.05.14, 19:11:57] \u0000+91\u000080878\u000029911\u0000: Hahahahaa\\[06.05.14, 20:47:41] \u0000+91\u000080878\u000029911\u0000: Wer isch das?????\\[06.05.14, 20:59:42] \u0000+91\u000080878\u000029911\u0000: Wies │   s .[[00.02.17, 10:56:38] Ha41\u00007\u0000\u0000\u0000\u0000\u0000\u00008\u0000\u0000\u0000\u0000\u0000 \u0000e07.02.18, 18:58:38] Ha41\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u00008\u0000\u0000\u0000:\u0000 \u0000ae        00.02.18, 10:58:38] Ha41\u00007\u0000\u0000\u0000\u0000\u0000\u00008\u0000\u0000\u0000:\u0000 \u0000ar ss h ses  [[[[[00.02.17, 10:48:38] Ha41\u00007\u0000\u0000\u0000\u0000\u0000\u00008\u0000\u0000\u0000\u0000\u0000 \u0000ae   │ loss: 1.61666\n",
            "└───────INDEX──────────BOOK NAME────┴────────────────────────────────────────────────────────────────────────────────────────────TRAINING SEQUENCE─────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────PREDICTED SEQUENCE────────────────────────────────────────────────────────────────────────────────────────────┴────LOSS─────┘\n",
            "\n",
            "TRAINING STATS: batch 46/102 in epoch 2,     batch loss: 1.61904, batch accuracy: 0.49425\n",
            "\n",
            "Validating on chat_felix.txt                               \n",
            "\n",
            "VALIDATION STATS:                                  loss: 1.69998,       accuracy: 0.47537\n",
            "\n",
            "0%                                        Training on next 50 batches                                        100%\n",
            "=================================================================================================================\n",
            "12000000 (epoch 2) chat_soziologie.txt │ pis anders...\\[05.06.14, 10:40:10] Papa: Ich bi oeppe i 40 min dezue parad , gat das?\\[05.06.14, 10:45:29] Holzmano: Ja dasch guet! Ok ich gib der alles wod muesch froege!\\\\Flugnummere 7242132411236\\V │ as en      ..[22.02.17, 10:19:27] Hapa: \u0000ch diede earssd 0aa  eer e aas e d dae aas  [07.02.17, 10:19:27] Holzmano: Ha aa  h aeer  [s ash deeeder sn e  eee seer h dae  e  [[eeee e er  se0.00.1.1 7 11: │ loss: 2.00434\n",
            "12000200 (epoch 2) chat_soziologie.txt │ 17, 15:51:36] Paula Guzman: Voll\\[19.11.17, 15:52:10] Holzmano:  gottseidank!\\[19.11.17, 23:46:22] Paula Guzman: Haesch es uf dae Zug gschaft?\\[19.11.17, 23:46:49] Holzmano: Loogo\\[19.11.17, 23:47:06] │ 7, 10:00:00] Hapla Guzman: Herl  12.02.17, 11:19:27] Holzmano: Hael        e [02.02.17, 10:29:27] Hapla Guzman: Hah  h ar ie de  saeeaa h     [17.02.17, 10:19:22] Holzmano: Hae e a02.02.17, 13:16:27]  │ loss: 1.23586\n",
            "12000400 (epoch 2) chat_soziologie.txt │ 04.17, 16:43:17] Julia Soziologie: I'm in for glueewii :P settmer vllt auno uf de fb poste denn\\[24.04.17, 16:44:49] Fabienne Soziologie: Falls ich (oder susch wer) no bi de aessbar verbii chume, froe │ 2.17, 11:26:22] Helia Soziologie: Ich dc dae deee  e  e [air  er aoe  en d de eersaeiae   der   12.02.17, 11:19:22] Helixnn  Soziologie: \u0000asi  ach due   aoe h der  ao deedersu   en dor e  dh e   aae   │ loss: 1.87033\n",
            "12000600 (epoch 2) chat_soziologie.txt │ 36:08] Felix: Hett ersch 26 eine ab winti..\\[14.06.14, 08:36:28] Felix: Aso miteme zweier chani uf seae, han denn abr ken aschluss\\[14.06.14, 08:39:48] Holzmano: Milchrampe\\[14.06.14, 08:40:13] Felix: │ 4:04] Helix: \u0000ah  ar  h d[ di   auedeed   .[02.02.17, 10:19:27] Helix: \u0000n  de    raeer   ahan  de eir   aae eer  aue sar en h ee   02.02.17, 10:19:27] Holzmano: Han h  s er 22.02.17, 10:26:27] Helix:  │ loss: 1.61201\n",
            "└───────INDEX────────────BOOK NAME─────┴────────────────────────────────────────────────────────────────────────────────────────────TRAINING SEQUENCE─────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────PREDICTED SEQUENCE────────────────────────────────────────────────────────────────────────────────────────────┴────LOSS─────┘\n",
            "\n",
            "TRAINING STATS: batch 96/102 in epoch 2,     batch loss: 1.65509, batch accuracy: 0.48165\n",
            "\n",
            "Validating on chat_felix.txt                               \n",
            "\n",
            "VALIDATION STATS:                                  loss: 1.64870,       accuracy: 0.48235\n",
            "\n",
            "┌───────────────────────────────────Generating random text from learned state───────────────────────────────────┐\n",
            "aennndere derer were der dere dere die gen sche se di gunde dere sch er si ger sie gere der dae der \n",
            "└───────────────────────────────────────────────End of generation───────────────────────────────────────────────┘\n",
            "\n",
            "0%                                        Training on next 50 batches                                        100%\n",
            "=================================================================================================================\n",
            "14000000 (epoch 3) chat_saunsch.txt │ assiter ... Kitsch\\\u0000[15.10.14, 21:36:52] Papa: \u0000Bild weggelassen\\[20.10.14, 20:10:58] Papa: Stinji isijdu usswaerts?\\[20.10.14, 20:51:43] Holzmano: Jaja...\\[20.10.14, 20:51:58] Papa: Ly\\[20.10.14, 22: │ e      s.. [an  h  [00.02.17, 10:46:48] Hapa: \u0000Bild weggelassen [00.02.17, 10:44:42] Hapa: \u0000o     gs    ege  ins    [06.02.17, 10:46:48] Holzmano: Ha   ...[00.02.17, 10:44:38] Hapa: \u0000a [00.02.18, 18:4 │ loss: 1.26006\n",
            "14000200 (epoch 3) chat_saunsch.txt │ isch behindi\\[20.11.17, 18:17:06] Holzmano: Du gell das isch bi dir zum entscheide...wennsichs erzwunge aafuehlt denn wartisch bises sich besser aafuehlt. Kei stress...mir hend ziit\\[20.11.17, 18:17:4 │ sch dir      00.01.17, 17:47:22] Holzmano: Haese   ses asch dinse  sie dr   h     ...ir    h  an  ied  ansee  i de   aee    h din   aceh dir    dn  e  i .wannsie    ...[n dan  di    00.02.17, 10:46:42 │ loss: 1.87795\n",
            "14000400 (epoch 3) chat_saunsch.txt │ dbhb ond mini ufgab\\[26.04.17, 21:18:24] Christina  Soziologie: ah han gmeint mini \\[26.04.17, 21:18:45] Christina  Soziologie: aber wenn ihr oeppis waend schriibe chanis nacher ufelade\\[26.04.17, 21: │  e esnd dec  gn e e 00.02.17, 10:44:48] Hhristina  Soziologie: Is dan deer   dec  se00.02.17, 12:46:58] Hhristina  Soziologie: Is r dern ds  de ee  eeer  woh  n ershon   aoeh   sn       00.02.17, 10:4 │ loss: 1.41734\n",
            "14000600 (epoch 3) chat_saunsch.txt │  noeis xaeppi\\\u0000[08.07.14, 08:22:18] Vera: \u0000Bild weggelassen\\[08.07.14, 08:23:29] Mama: Wer isch de das?\\[08.07.14, 08:23:57] Vera: Eine womer 'keneglernt' hend\\[08.07.14, 08:24:09] Mama: Hei waer het  │ go r  ga  eee [07.02.17, 10:44:38] Pera: \u0000Bild weggelassen [00.02.17, 10:46:48] Hama: \u0000er ssch de ses  [06.02.18, 10:46:38] Hera: \u0000s   see r asen   i    [[an   00.02.17, 10:46:48] Hama: \u0000ai ieer san g │ loss: 1.38118\n",
            "└───────INDEX──────────BOOK NAME────┴────────────────────────────────────────────────────────────────────────────────────────────TRAINING SEQUENCE─────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────PREDICTED SEQUENCE────────────────────────────────────────────────────────────────────────────────────────────┴────LOSS─────┘\n",
            "\n",
            "TRAINING STATS: batch 44/102 in epoch 3,     batch loss: 1.58351, batch accuracy: 0.50050\n",
            "\n",
            "Validating on chat_felix.txt                               \n",
            "\n",
            "VALIDATION STATS:                                  loss: 1.61470,       accuracy: 0.49035\n",
            "\n",
            "0%                                        Training on next 50 batches                                        100%\n",
            "=================================================================================================================\n",
            "16000000 (epoch 3) chat_soziologie.txt │ fuers psueexli!\\Gats mim leere?\\Imfallt lust haej, es git umdi 4i oeppis feins - muesstijdi aber gli bim muems mal maelde ( home-tel. )\\\u0000[15.05.15, 12:19:25] Papa: \u0000Audio weggelassen\\[15.05.15, 15:01: │ eer  daee      [rn  det dann   [   el  dae  dant  ds den dn   s  gnreen iarn  i aaet       sner deaedin dier  mir sin e  suaae r er .W [[08.02.17, 10:48:08] Hapa: \u0000Bid   weggelassen [08.02.18, 10:46:0 │ loss: 2.08512\n",
            "16000200 (epoch 3) chat_soziologie.txt │ .11.17, 14:55:53] Paula Guzman: Mir sind am Kuers bespreche gsi, ich han die ganz ziit versuecht vorschlaeg z\u0000finde und er luegt mich ah und seit wenig bis nuet. Aber meh gsehts im ah, das was ich sae │ 02.17, 10:46:00] Hapla Guzman: Wan scnd du doet  iir ee h  se   dnh dan de  send iu   gor  enh  dorl h is  du[ied rwnd ws diete dith dn dnd donn dird  din ioer .dber dir dee    dn dn  das iee inh dinn │ loss: 1.85265\n",
            "16000400 (epoch 3) chat_soziologie.txt │ ppe usw \\[28.04.17, 00:02:49] Christina  Soziologie: danke eu fuer eui hilf! sust waer nuet zstand cho \\[28.04.17, 06:16:31] Nils Soziologie: danke euch foers ufruhme ond heischleppe \\[28.04.17, 09:06 │ eersn ede08.02.17, 11:38:56] Christina  Soziologie: Ias   ss daer ds  dan   [ce  deer doer du  n  dho de08.02.17, 11:38:33] Coli Soziologie: Ias   ss h daer  in  e  rsnd dann h i eerse08.02.17, 11:38: │ loss: 1.54030\n",
            "16000600 (epoch 3) chat_soziologie.txt │ voll d post ap mitso truemeli, bierseeligem palaver und warmer, abgasjwangerer sumerluft.\\[09.08.14, 23:24:06] Felix: Pfus guet lysm\\[09.08.14, 23:25:47] Felix: De turm ijso jraeaeaeaeg, das glaubj ga │ orl desae  dn een   deeet r   din          dae  er dnd dee er  dnee  eende    doe r eee .[08.02.17, 10:48:08] Helix: \u0000aae deet da  e[08.02.18, 18:48:06] Helix: \u0000ansee  ds   da n         das ieasse dee │ loss: 2.13385\n",
            "└───────INDEX────────────BOOK NAME─────┴────────────────────────────────────────────────────────────────────────────────────────────TRAINING SEQUENCE─────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────PREDICTED SEQUENCE────────────────────────────────────────────────────────────────────────────────────────────┴────LOSS─────┘\n",
            "\n",
            "TRAINING STATS: batch 94/102 in epoch 3,     batch loss: 1.51800, batch accuracy: 0.51635\n",
            "\n",
            "Validating on chat_felix.txt                               \n",
            "\n",
            "VALIDATION STATS:                                  loss: 1.58196,       accuracy: 0.49859\n",
            "\n",
            "0%                                        Training on next 50 batches                                        100%\n",
            "=================================================================================================================\n",
            "18000000 (epoch 4) chat_saunsch.txt │ keiten Gottes rechnet, erlangt die echte Heiterkeit des Lebens\"! Haenddi UMG - dini Elstere\\[18.02.16, 08:35:35] Holzmano: Danke ihr sind di beschte!!! Han eu so unbeschriblich gern und freue mich uf  │ e     duee   iutht    ds es   de  sih  nsann   e   den moner    Wan     sndrs uee  gs   n   20.02.17, 10:57:57] Polzmano: Has e ss  dccd se sin h    !![ah di gc dnder h  cei h den  gnd duene gith dn d │ loss: 2.10106\n",
            "18000200 (epoch 4) chat_saunsch.txt │ ch chume?!\\[24.11.17, 10:56:52] Paula Guzman: Das chan ich dir noed saege. Wenn du lust haesch zum am Stir it go tanze denn chunsch und wenn noed denn isch es au ok. \\As you like...\\Waeg mir musch noe │ h dhoe   ![27.02.17, 17:27:57] Hapla Guzman: Has ghon gch den go d sihn  .Wann deegeec gant h due mu miin gc geedie   ginn ihoed h dnd wern do r senn icch di gu gee w[n i  egiee .. [es  sit die h do   │ loss: 1.70162\n",
            "18000400 (epoch 4) chat_saunsch.txt │ rasse bar ! mi gsellt sich morn abe vo 17 - 23 Uhr ar stauffacherstr. 195. Es git sound und getraenk  froeie mi wennder choemet\\[10.05.17, 14:30:13] Melis Soziologie: isch das dini wg simon?:)\\[10.05. │      dis d aitheen   gi h dir  duersord  d a0 dn  du dien  eeh       d0.00Ws wen di e  dnd wen  nn eiuueen  sitienn  r dhoen r  00.02.17, 12:58:58] Calis Soziologie: Icch das men  geedcn      [00.02.1 │ loss: 2.01649\n",
            "18000600 (epoch 4) chat_saunsch.txt │ uessti en tag spoeter cho..\\[27.08.14, 21:26:00] Felix: Goend ez 35 uf de zuug\\[27.08.14, 22:03:36] Mama: Mir haend zu dim 20. woelle ga aesse am suntag. Irgendwie haemmer gmeint, du choemisch am sams │ er   ggn din giaen   dhoe .[20.02.17, 10:47:58] Pelix: \u0000uet  wi g0 dn dirsueee[00.02.17, 19:47:58] Pama: \u0000in san   suege  s0.0Weer   sengu    gu gied g .dc     ie sant  r dear    daeghoen   h du min   │ loss: 1.88825\n",
            "└───────INDEX──────────BOOK NAME────┴────────────────────────────────────────────────────────────────────────────────────────────TRAINING SEQUENCE─────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────PREDICTED SEQUENCE────────────────────────────────────────────────────────────────────────────────────────────┴────LOSS─────┘\n",
            "\n",
            "TRAINING STATS: batch 42/102 in epoch 4,     batch loss: 1.48075, batch accuracy: 0.52608\n",
            "\n",
            "Validating on chat_felix.txt                               \n",
            "\n",
            "VALIDATION STATS:                                  loss: 1.55575,       accuracy: 0.49840\n",
            "\n",
            "0%                                        Training on next 50 batches                                        100%\n",
            "=================================================================================================================\n",
            "20000000 (epoch 4) chat_soziologie.txt │ zgruende und soell d moeglixkeite bi u-cloud usboue\\Xeej du besseri moeglixkeite? Vill fotene hani jo waegtaa\\[21.05.16, 10:14:20] Holzmano: Tue der dropbox app abelade, denn alli fotene drufflade(upl │    et   and do nl sesirnee       dindn h ee sn e e     eaaesin e   girnee        [irl wue    dan  da deene n  08.04.18, 11:55:55] Holzmano: Woetse  de  ee  sn esner     dann dule gue    de e eee   nee │ loss: 2.33742\n",
            "20000200 (epoch 4) chat_soziologie.txt │ de richtigi Weg und er wuerdi ois beidne am meischte Chummer erspare \u0000 little did i know!\\\\Paula ich han dich verfluecht gern! Ich vermisse dich extrem und wuerd dich gern wider gseh! Gliichzitig bin  │ ennanh  n  den end dn deer   hn  hinne  dn dirneh  ndhae er di  en  e[iee  i se  wnseiee [[anse dnh dan denh dor eeeth  den   Wch hor ec e denh di   n dnd deer  de h den  dee r dee   Wuaech  n   din d │ loss: 2.22445\n",
            "20000400 (epoch 4) chat_soziologie.txt │ tudis\\[17.05.17, 15:26:27] Sadhbh Soziologie: Aber ja, selber tschuld\\[17.05.17, 15:26:53] Sadhbh Soziologie: Ha etz aber au no nuet gseh woni umbedingt wott gseh oder lose\\[17.05.17, 15:27:35] Xenia  │  e    28.02.17, 11:55:55] Ctmhbh Soziologie: Iher da  dcnler dich ed  00.02.18, 11:55:55] Ctmhbh Soziologie: Iahah  guer du do doer de   deed dn er     dee  de   dn r dae   13.05.15, 11:55:55] Cenia S │ loss: 1.44851\n",
            "20000600 (epoch 4) chat_soziologie.txt │ :24] Papa: \u0000Bild weggelassen\\\u0000[09.10.14, 21:36:10] Papa: \u0000Bild weggelassen\\[09.10.14, 21:43:08] Papa: ... Und da no e sakral iiwandfreii frisur\\\u0000[09.10.14, 21:43:31] Papa: \u0000Bild weggelassen\\[10.10.14, │ 55] Hapa: \u0000Bild weggelassen [[08.02.17, 11:45:55] Hapa: \u0000Bild weggelassen [08.02.18, 11:55:55] Hapa: \u0000.. dnd dasso dssin  n ss e   ae   gueg e  [08.02.18, 11:45:55] Hapa: \u0000Bild weggelassen [08.02.17,  │ loss: 1.01579\n",
            "└───────INDEX────────────BOOK NAME─────┴────────────────────────────────────────────────────────────────────────────────────────────TRAINING SEQUENCE─────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────PREDICTED SEQUENCE────────────────────────────────────────────────────────────────────────────────────────────┴────LOSS─────┘\n",
            "\n",
            "TRAINING STATS: batch 92/102 in epoch 4,     batch loss: 1.47145, batch accuracy: 0.52997\n",
            "\n",
            "Validating on chat_felix.txt                               \n",
            "\n",
            "VALIDATION STATS:                                  loss: 1.53200,       accuracy: 0.50968\n",
            "Saved file: drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_models/rnn_train_1548172463-20000000\n",
            "\n",
            "0%                                        Training on next 50 batches                                        100%\n",
            "=================================================================================================================\n",
            "22000000 (epoch 5) chat_saunsch.txt │ 7:40:17] Papa: Verwuetjdi noed, egal - es andersmal klappets!\\[13.06.16, 17:41:03] Papa: Sori falje adraessat\\[13.06.16, 23:16:59] Holzmano: \\[13.06.16, 23:35:19] Papa: Seesack ok?\\De Alubode xaj ja u │ :49:59] Papa: \u0000orleesee  mo d  ds e g di duder  ellgeieeer   [27.06.18, 10:47:49] Papa: \u0000o   guel  au      n [23.06.18, 10:48:49] Polzmano: Ha17.02.17, 10:49:59] Papa: \u0000oil  heae  [  sblee eraeeeaa an │ loss: 1.73159\n",
            "22000200 (epoch 5) chat_saunsch.txt │ Guzman: \\[06.12.17, 22:49:13] Paula Guzman: Idemfall haets scho d\u0000rundi gmacht.\\[06.12.17, 22:56:34] Holzmano: Nope...aber mini mam hett halt gwuesst dasich mit dir abgmacht han ... naagfroegt und gra │ uzman: Ha17.11.17, 17:07:07] Haula Guzman: Hc   all iant  goho gee ed  gealht .[17.12.17, 17:07:07] Holzmano: Hee  .. ler dit  dic datt dan  geaetse des  h dit de  dueeecht dan d...doc  aeete dnd deau │ loss: 1.49581\n",
            "22000400 (epoch 5) chat_saunsch.txt │ hriebig:\\\u0000[20.06.17, 18:51:43] C\u0000line Gloor: \u0000Bild weggelassen\\[20.06.17, 19:14:13] C\u0000line Gloor: Ah ja ich mach uebrigens en couscous salat und en fetacake:)\\[20.06.17, 19:37:06] Nils Soziologie: \\[2 │ ten e   [[17.15.17, 18:58:58] Chline Gloor: IBild weggelassen [27.02.18, 10:58:58] Chline Gloor: Ib da dsh dichtdnde c    ds dhoe h e monl  dnd di durtehte   [27.05.18, 10:58:58] Cols Soziologie: Ia07 │ loss: 1.26741\n",
            "22000600 (epoch 5) chat_saunsch.txt │ choemeds zu ois, mir choched essed und mached forum bis ca 2200\\[17.11.14, 11:16:11] Mama: Klar!!\\[17.11.14, 11:19:10] Felix: Kuhl\\[17.11.14, 13:35:15] Felix: Brucht oeppr sauto am nami?\\[17.11.14, 13 │ h  ner  muegee  dit dhoeht  di e  dnd deche  duede dit ihld ... 00.12.17, 17:47:47] Pama: Wais   [27.02.17, 17:07:07] Pelix: \u0000aet  20.02.17, 10:47:47] Pelix: \u0000iueht deppeesone  du moe   [27.02.17, 10: │ loss: 1.58159\n",
            "└───────INDEX──────────BOOK NAME────┴────────────────────────────────────────────────────────────────────────────────────────────TRAINING SEQUENCE─────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────PREDICTED SEQUENCE────────────────────────────────────────────────────────────────────────────────────────────┴────LOSS─────┘\n",
            "\n",
            "TRAINING STATS: batch 40/102 in epoch 5,     batch loss: 1.51223, batch accuracy: 0.52065\n",
            "\n",
            "Validating on chat_felix.txt                               \n",
            "\n",
            "VALIDATION STATS:                                  loss: 1.51988,       accuracy: 0.50929\n",
            "\n",
            "0%                                        Training on next 50 batches                                        100%\n",
            "=================================================================================================================\n",
            "24000000 (epoch 5) chat_soziologie.txt │  Holzmano: Iwenb\\[26.07.16, 12:44:29] Holzmano: *iwenn\\[26.07.16, 12:53:24] Papa: Ok am spoetere Nami luetider vode FeWo haer aa. \\Ly\\[26.07.16, 12:53:56] Holzmano: Aha ok kes problem...schribmer zers │ Holzmano: Hch  er27.04.18, 10:42:25] Holzmano: Ha er   18.04.18, 10:42:25] Hapa: \u0000k iu doaet    aoc  aaet     dore arraegaet du .D[e [28.04.18, 10:44:44] Polzmano: Hl  ae iei graeee  .. ehoiceer dui c │ loss: 1.61003\n",
            "24000200 (epoch 5) chat_soziologie.txt │ zman: Ok, denn weiss ich au noed\\[08.12.17, 09:25:39] Holzmano: Villicht macht das au mis handy...\\\u0000[08.12.17, 11:02:42] Holzmano: \u0000Bild weggelassen\\[08.12.17, 11:15:39] Paula Guzman: And now?\\[08.12. │ man: Hk  dann derns dch du doed  17.11.17, 10:27:23] Holzmano: Holl  h  dilhe des gu dit gand  .. [[17.02.17, 10:27:35] Holzmano: HBild weggelassen [29.02.17, 10:22:25] Haula Guzman: Hld so i [17.11.1 │ loss: 0.96466\n",
            "24000400 (epoch 5) chat_soziologie.txt │ tem andere\\[13.07.17, 14:13:43] Sadhbh Soziologie: Am beste efach mal verzele\\[13.07.17, 15:05:18] Fabienne Soziologie: Ja ich glaub das hemer bi de saiv oder bi mir besproche gha, irgendwas dass es n │    dud r   11.04.18, 11:22:25] Ctmhbh Soziologie: Il din e sr eh dil dor erl  11.04.18, 11:22:33] Cebienne Soziologie: Ia ach daausedes iat   disdersontedeer disdit dii eoeht seae ds e   ie ses  ii io │ loss: 1.45056\n",
            "24000600 (epoch 5) chat_soziologie.txt │ .02.15, 15:12:02] Papa: \u0000Bild weggelassen\\\u0000[02.02.15, 15:12:09] Papa: \u0000Bild weggelassen\\\u0000[02.02.15, 15:12:18] Papa: \u0000Bild weggelassen\\[02.02.15, 15:12:24] Felix: \\\u0000[02.02.15, 15:12:27] Papa: \u0000Bild weg │ 04.18, 10:44:45] Papa: \u0000Bild weggelassen [[08.04.18, 10:44:45] Papa: \u0000Bild weggelassen [[08.04.18, 10:44:44] Papa: \u0000Bild weggelassen [28.04.18, 10:44:45] Pelix: \u0000[[28.04.18, 10:44:45] Papa: \u0000Bild wegg │ loss: 0.60887\n",
            "└───────INDEX────────────BOOK NAME─────┴────────────────────────────────────────────────────────────────────────────────────────────TRAINING SEQUENCE─────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────PREDICTED SEQUENCE────────────────────────────────────────────────────────────────────────────────────────────┴────LOSS─────┘\n",
            "\n",
            "TRAINING STATS: batch 90/102 in epoch 5,     batch loss: 1.41652, batch accuracy: 0.54255\n",
            "\n",
            "Validating on chat_felix.txt                               \n",
            "\n",
            "VALIDATION STATS:                                  loss: 1.49827,       accuracy: 0.51528\n",
            "\n",
            "┌───────────────────────────────────Generating random text from learned state───────────────────────────────────┐\n",
            "erussten waere schaen en wir denn den wer dinn derne und wenn der wern ick de noed wie schlachen wer\n",
            "└───────────────────────────────────────────────End of generation───────────────────────────────────────────────┘\n",
            "\n",
            "0%                                        Training on next 50 batches                                        100%\n",
            "=================================================================================================================\n",
            "26000000 (epoch 6) chat_saunsch.txt │ , 14:15:28] Papa: Bi offline - goend use...\\[03.12.16, 19:40:55] Holzmano: Freue mi mega! Bis bald\\[10.12.16, 12:07:33] Papa: Hei nimmjna es xopfab mit? Mrc\\[10.12.16, 12:07:44] Holzmano: Hae?\\[10.12. │  10:43:49] Papa: \u0000i de ee   a daen  sn  .. [26.02.17, 10:47:47] Polzmano: Haass sitdir   Din his   03.02.17, 10:47:47] Papa: \u0000ai doe ee  as aoeeeeesit  Dithe03.00.17, 10:43:47] Polzmano: Haha [27.02.1 │ loss: 1.44681\n",
            "26000200 (epoch 6) chat_saunsch.txt │  Guzman: Haha, her damit\\[08.12.17, 19:55:05] Holzmano: Als Chind amene andere Chind en \u0000pfel so fescht an Chopf gschmisse dases dNase broche gha hett...\\[08.12.17, 19:55:28] Paula Guzman: \\[08.12.17, │ Guzman: Haha  dan das   [27.12.17, 10:37:47] Holzmano: Hhloghan  dm r  auder  shan ess sBiarlsi galth  db dhaepeseeh et   ses   aeec  siacht seansen  .. [27.12.17, 10:37:47] Raula Guzman: H[17.12.17,  │ loss: 1.48679\n",
            "26000400 (epoch 6) chat_saunsch.txt │ ge obs no het? (die sind uuh schnell usverchauft)\\[23.08.17, 17:37:12] C\u0000line Gloor: Danke Nils! \\Und Fabienne: leider noed, gha grad uf e abigwanderig\\[23.08.17, 17:37:37] Fabienne Soziologie: Cool:) │ e wne ao den  Isee sind dn  dchoi l sn er honn   [27.03.18, 10:33:49] Chline Gloor: Iaske sec   D[nd dase   e  Dainer do d  deansead sn disueg ald      23.06.18, 10:43:49] Cebienne Soziologie: Ihol    │ loss: 1.62851\n",
            "26000600 (epoch 6) chat_saunsch.txt │  oeppe am foifi\\[22.03.15, 20:46:53] Papa: 27./28. juni fuer familienwochenende auch ok?\\Bitte alle Antw.\\[22.03.15, 21:37:34] Vera: Hamma papa\\[24.03.15, 12:45:59] Papa: Cool\\Und laenz?\\[24.03.15, 12 │ deppersu saene  03.06.18, 10:43:49] Papa: \u0000B.0 [.01aee aaer dae      erh      rsu h dee [il   dule sbdei [23.06.18, 10:43:49] Pera: \u0000ah elaase  03.06.18, 10:43:49] Papa: \u0000hol   d waeg   [23.06.18, 10: │ loss: 1.71800\n",
            "└───────INDEX──────────BOOK NAME────┴────────────────────────────────────────────────────────────────────────────────────────────TRAINING SEQUENCE─────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────PREDICTED SEQUENCE────────────────────────────────────────────────────────────────────────────────────────────┴────LOSS─────┘\n",
            "\n",
            "TRAINING STATS: batch 38/102 in epoch 6,     batch loss: 1.42659, batch accuracy: 0.54455\n",
            "\n",
            "Validating on chat_felix.txt                               \n",
            "\n",
            "VALIDATION STATS:                                  loss: 1.48303,       accuracy: 0.51885\n",
            "\n",
            "0%                                        Training on next 50 batches                                        100%\n",
            "=================================================================================================================\n",
            "28000000 (epoch 6) chat_familie.txt │ er erst gaegdi 22h dehei! \\Immer willkome!\\[08.07.17, 10:00:50] Holzmano: Aso weg dem Beamer...er muesst erach FullHD choene aazeige und en Hdmi aaschluss ha\\[08.07.17, 10:01:21] Holzmano: Ah und sind │ r ai   deete  g0. aa     D[c er deel   er [24.04.14, 10:49:45] Polzmano: Hh  denede asisser .. [ diesse di ehtdael eeshoent dm    e gnd ds see  gm  h ieseden 19.04.18, 10:49:45] Polzmano: Hhadfd dcnd  │ loss: 1.83372\n",
            "28000200 (epoch 6) chat_familie.txt │ sched 13:00 und 14:00 chumi di go hole und den goemmer zeme ufe\\[09.12.17, 20:39:03] Paula Guzman: All right.\\[09.12.17, 20:57:20] Holzmano: Und scho bim taessert?!\\\u0000[09.12.17, 20:58:13] Paula Guzman: │ sh   d..0  dnd d0.   ihoe  deemeedael dnd denndeeteer duierdn   27.02.17, 10:49:49] Raula Guzman: Hhsesac e .[17.12.17, 10:47:47] Holzmano: Hnd dcho ges deetse    ![[19.02.17, 10:49:49] Raula Guzman:  │ loss: 1.28703\n",
            "28000400 (epoch 6) chat_familie.txt │ ammmer! Danke!\\\u0000[14.09.17, 09:28:10] Sadhbh Soziologie: \u0000Bild weggelassen\\[14.09.17, 10:04:51] Fabienne Soziologie: Ja also ich han vor ihn no zfroege die wuche/das we. Wahrschiinli chan er ja nur a e │ haear  Daske  [[27.04.18, 10:49:49] Ftmhbh Soziologie: \u0000Bild weggelassen [14.04.18, 11:49:49] Cebienne Soziologie: \u0000a allo dch dan dor dcr do duaeene aee seehe  as ier desr  h en   ghon dn sa aoe dbdr │ loss: 1.30177\n",
            "28000600 (epoch 6) chat_familie.txt │ stern im Irak. Dies ist wirklich ein SOS-Ruf. Moege Gott Euch segnen.\\\\\\Original:\\\\URGENT PRAYER REQUEST from my friend Andrew White in Baghdad.\\Below is the news from three days ago, but today the ne │ se  eds dc ee Dan  isc dee eich din diri aer Dirne dret ds h dihee  .[[   g e    [[n eieeSeesee SeEE SEESEeeeii maeete ibdeaneseen  dn diseeen  .inl egscgeansode iraeedeeinnees  in   die deeee deensod │ loss: 3.04667\n",
            "└───────INDEX──────────BOOK NAME────┴────────────────────────────────────────────────────────────────────────────────────────────TRAINING SEQUENCE─────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────PREDICTED SEQUENCE────────────────────────────────────────────────────────────────────────────────────────────┴────LOSS─────┘\n",
            "\n",
            "TRAINING STATS: batch 88/102 in epoch 6,     batch loss: 1.42629, batch accuracy: 0.54817\n",
            "\n",
            "Validating on chat_felix.txt                               \n",
            "\n",
            "VALIDATION STATS:                                  loss: 1.46594,       accuracy: 0.52917\n",
            "\n",
            "0%                                        Training on next 50 batches                                        100%\n",
            "=================================================================================================================\n",
            "30000000 (epoch 7) chat_saunsch.txt │ Nope\\[14.10.17, 20:18:29] Holzmano: No underwegs...\\[14.10.17, 20:18:28] Papa: Ufrume?\\[14.10.17, 20:18:36] Holzmano: Noni mal aacho\\[14.10.17, 20:18:51] Papa: Waaas? No am gotthd?\\[14.10.17, 20:19:26 │ ee   23.02.17, 10:43:46] Polzmano: Heesed r eie .. [27.02.17, 10:47:47] Papa: \u0000n  e   [23.02.14, 10:44:46] Polzmano: Hee  gic gu h  [23.02.17, 10:43:46] Papa: \u0000as a  Dieiu saee    [23.02.14, 10:44:46] │ loss: 1.23995\n",
            "30000200 (epoch 7) chat_saunsch.txt │ : Srry han bis etz Bandprob gha..\\Nei isch doch ned egal! Villicht wett si dich ganz bewusst ned als Chefin aarede - sondern als Kollegin! Oder epper wosi wuekli gern hett und schetzt...\\Vergibere und │  Hoia aan dis dsz gis  reeegea ..[ei asch de h dod ds    Dorl  h  den  dicde h denz giseese dod duleghanf  du     s si n r  dulegaele     Dker ssper se   dierei gert dett dnd dihoi   .. [er ene   snd  │ loss: 2.07890\n",
            "30000400 (epoch 7) chat_saunsch.txt │ ir bitte no die mailadresse, ueber die ihr d mails an fachvereinsvorstand empfange moechted (v.a. falls noed d uzh-adresse)\\[20.09.17, 17:04:16] \u0000+41\u000079\u0000640\u000087\u000008\u0000: Fuer mich isch d'uzh adresse tiptop │ n din   so dee sicn  e   e  dnber dee ssr desic   iuddreh er     er eeegedn erlie dirnht   duo   irel  io d desn e uee  en [27.06.18, 10:44:46] C+41\u000079\u000044\u0000\u000019\u000018\u0000: \u0000aer dith dsch dase edu    e aeee  e │ loss: 2.12684\n",
            "30000600 (epoch 7) chat_saunsch.txt │ di umg...\\Aber logemoleplease\\[25.06.15, 08:46:10] Felix: Inre 4tel stund hani prueefig!\\[25.06.15, 09:51:13] Papa: Mir taenked adix!\\[25.06.15, 09:52:25] Vera: Han gaz fescht fuer dich bettet! \\[25.0 │   gn e.. [ber daee     ei    [23.06.14, 10:41:46] Helix: Hc   s    soaeg dae  graetee   [27.06.14, 10:41:46] Hapa: \u0000ir doende  sue   [23.06.14, 10:41:46] Pera: hah dan guish  duer di h disz    D[07.06 │ loss: 1.63412\n",
            "└───────INDEX──────────BOOK NAME────┴────────────────────────────────────────────────────────────────────────────────────────────TRAINING SEQUENCE─────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────PREDICTED SEQUENCE────────────────────────────────────────────────────────────────────────────────────────────┴────LOSS─────┘\n",
            "\n",
            "TRAINING STATS: batch 36/102 in epoch 7,     batch loss: 1.37943, batch accuracy: 0.56227\n",
            "\n",
            "Validating on chat_felix.txt                               \n",
            "\n",
            "VALIDATION STATS:                                  loss: 1.45990,       accuracy: 0.52846\n",
            "\n",
            "0%                                        Training on next 50 batches                                        100%\n",
            "=================================================================================================================\n",
            "32000000 (epoch 7) chat_familie.txt │ sen\\\u0000[18.02.18, 14:26:38] Holzmano: \u0000Bild weggelassen\\\u0000[18.02.18, 17:46:35] Papa: \u0000Bild weggelassen\\[23.02.18, 12:04:49] Papa: Xemerois im Manor-rest? 12.30h\\[23.02.18, 12:05:02] Holzmano: 12:45 hani  │ en [[09.04.17, 10:49:49] Colzmano: \u0000Bild weggelassen [[09.04.17, 10:49:19] Capa: \u0000Bild weggelassen [09.06.18, 10:08:19] Hapa: \u0000en      gs sarg   aise D0.00.ae02.04.14, 10:18:19] Holzmano: H..0. dan  d │ loss: 1.05901\n",
            "32000200 (epoch 7) chat_familie.txt │ foniere rockt...! Igendwe isch das smsle au isi unbefridigend und vill weniger persoenlich...\\\\Gell das lied isch ebe geil! Und i dem moment - exakt jetzt - ohni dasich mit mim brueeder gredet han sin │ aed r  deehe .. !Dch   iissch des giici du dsc gnderaege     gnd doll denn    darse nte h .. [1rrleses geebedsch die dein  Dnd dcce  sirm   d di  e datz  d deee des  h dit dit diuegser deaie  den dind │ loss: 2.06886\n",
            "32000400 (epoch 7) chat_familie.txt │ ie om de sofa god ond ned om zoeri oder d uzh saelber.\\[25.09.17, 23:26:10] Nils Soziologie: ech ben hoet met de c\u0000line no chorz zaeme ghockt zom das diplom mol chli zgstalte:\\\u0000[25.09.17, 23:26:48] Ni │   ue sensi aldeeese  iod de su ne geer desn  dihnle   [14.04.18, 11:49:19] Cils Soziologie: \u0000hh dinndeen dir de fhai   do dhoen gumge aeaehe du  ses ieeeeeesir ghoi due egt   [[09.04.18, 10:19:39] Cil │ loss: 1.85283\n",
            "32000600 (epoch 7) chat_familie.txt │ hte: jung huebsch gschiid u interessant- alli drei! Ly\\[27.08.15, 13:21:48] Holzmano: Bruchedir sauto vom 1500 bis ca am 1800?!\\[27.08.15, 17:14:48] Papa: Xajes ha -\\Und poah gratuliere jraess... Has  │ t   ia d daete h deeht gegnd  er   ee   dble geu   De [24.04.18, 10:08:19] Holzmano: Hiuehe    dohe  gor s..0.dis dhuam s..0. ![29.04.18, 10:08:19] Papa: \u0000ena  aaea and drreeaead e     gaadne .. ian d │ loss: 1.96087\n",
            "└───────INDEX──────────BOOK NAME────┴────────────────────────────────────────────────────────────────────────────────────────────TRAINING SEQUENCE─────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────PREDICTED SEQUENCE────────────────────────────────────────────────────────────────────────────────────────────┴────LOSS─────┘\n",
            "\n",
            "TRAINING STATS: batch 86/102 in epoch 7,     batch loss: 1.38360, batch accuracy: 0.55910\n",
            "\n",
            "Validating on chat_felix.txt                               \n",
            "\n",
            "VALIDATION STATS:                                  loss: 1.44406,       accuracy: 0.53289\n",
            "\n",
            "0%                                        Training on next 50 batches                                        100%\n",
            "=================================================================================================================\n",
            "34000000 (epoch 8) chat_mama.txt │ :07] Papa: \\\u0000[26.10.18, 07:56:53] Papa: \u0000Bild weggelassen\\[26.10.18, 07:57:11] Papa: Da bini gad per Zuefall drufxoo\\[26.10.18, 07:57:21] Papa: \\[26.10.18, 10:26:31] Holzmano: so fertig! zit?\\[26.10.1 │ 49] Papa: \u0000[[27.02.17, 10:49:49] Papa: \u0000Bild weggelassen [27.02.16, 10:49:46] Papa: \u0000ashis  gae grrtauetell ga e   e[23.02.16, 10:41:46] Hapa: \u0000[07.02.16, 10:49:46] Polzmano: Ho gurt g !Dum  [27.02.17 │ loss: 1.11010\n",
            "34000200 (epoch 8) chat_mama.txt │ 3] Paula Guzman: Schlafen sie gut.\\[17.12.17, 00:18:40] Holzmano: Sooo ... jetz hani wider internet...gopf sorry!\\Min Abig isch easy gsi - wenn au ned de Brueller ehrlich gseit! Und am Mentig gits den │ ] Haula Guzman: Hohoie   min gaet.[22.12.17, 10:41:47] Holzmano: Ho o g.. aatz gan  geeer ds  r e  .. eepesi d   [ir dbeg dsch di   dee g denn du dod densiuegee  gi  ich deeh   Dnd dl serneg gen  genn │ loss: 1.58424\n",
            "34000400 (epoch 8) chat_mama.txt │ iologie: \\be mer werds chli knapp, choend gaern scho afoh\\\u0000[03.10.17, 21:11:22] Xenia Soziologie: \u0000Bild weggelassen\\[03.10.17, 21:11:43] Christina  Soziologie: \\[03.10.17, 21:11:57] \u0000+41\u000079\u0000640\u000087\u000008\u0000 │ ologie: h[erdir denn  dhoi aeae e dhoend deen  diho gb eee[26.00.17, 10:44:49] Cenia Soziologie: \u0000Bild weggelassen [26.02.17, 10:44:46] Christina  Soziologie: h[06.02.17, 10:44:46] C+41\u000079\u0000440\u000089\u000018\u0000: │ loss: 0.91557\n",
            "34000600 (epoch 8) chat_mama.txt │ 3:22:51] Vera: ich!\\[16.10.15, 17:55:57] Papa: Staubsuger\\Teppich\\Xuxitueechli\\Gschirr\\Psteck\\Vase\\[16.10.15, 17:56:45] Papa: Tabourettli\\Feldstecher\\Fixlintueexr\\[16.10.15, 17:56:58] Papa: Xleidrbueg │ :49:46] Fera: hch  [27.02.14, 10:44:46] Fapa: \u0000oiree n   [a  e h [     er h i [eehte  [[ erhe[ere [23.02.16, 10:44:46] Papa: \u0000oe  e    i [erl    h   [ae     e    [23.02.16, 10:44:46] Papa: \u0000ee   ieere │ loss: 1.94575\n",
            "└───────INDEX─────────BOOK NAME──┴────────────────────────────────────────────────────────────────────────────────────────────TRAINING SEQUENCE─────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────PREDICTED SEQUENCE────────────────────────────────────────────────────────────────────────────────────────────┴────LOSS─────┘\n",
            "\n",
            "TRAINING STATS: batch 34/102 in epoch 8,     batch loss: 1.35614, batch accuracy: 0.56372\n",
            "\n",
            "Validating on chat_felix.txt                               \n",
            "\n",
            "VALIDATION STATS:                                  loss: 1.43444,       accuracy: 0.53647\n",
            "\n",
            "0%                                        Training on next 50 batches                                        100%\n",
            "=================================================================================================================\n",
            "36000000 (epoch 8) chat_familie.txt │ Ich choenntmer jedefalls nid vorstelle das en andere besser weri fuer de job :) !\\\\So etz chasch du mir saege was ich sell mache ...!\\[28.02.14, 07:18:30] Rafiq of the Many Baumann: Haha du wuersch di │ ch hhaend  er dete acl  dod dor cerl  ues gs suder  siscer den  guer de sa ed) W![[t diz dhonch de sit dccte des dsh dchl diche d.. ![18.06.17, 10:00:07] Rafiq of the Many Baumann: Haha aa seer  h de  │ loss: 1.34101\n",
            "36000200 (epoch 8) chat_familie.txt │ \\[18.12.17, 14:57:34] Paula Guzman: Jap\\[18.12.17, 16:03:45] Paula Guzman: han oepe so haert muesse lache wie bii ken lee\\\\https://youtu.be/w4i6PcCuBA4\\[18.12.17, 16:08:35] Paula Guzman: Waens dich ii │  18.02.17, 10:00:06] Raula Guzman: Ha  [11.12.17, 10:07:07] Haula Guzman: Han depprsc gent  giess  ueeht uie gis gainaaite    e //wout.e.e//owccehhnrre[12.02.17, 10:07:16] Haula Guzman: Hasrn daeh dsc │ loss: 1.38156\n",
            "36000400 (epoch 8) chat_familie.txt │ nnt\\[10.10.17, 10:33:55] Toniba Sofa: ja voll :)\\[10.10.17, 10:34:00] Christina  Soziologie: isch im plan ufgschribe wieviel vo was mer bruchet?\\[10.10.17, 10:34:00] Toniba Sofa: und hemmer e uebergan │ t   19.02.17, 10:07:09] Coniba Sofa:  a doll s) [19.02.17, 10:07:09] Christina  Soziologie: hcch dc srasdun echoice dee err so dee dir diueht   [19.00.17, 10:07:09] Coniba Sofa:  nd dan er disnber end │ loss: 1.08934\n",
            "36000600 (epoch 8) chat_familie.txt │ ra: ich bi den direkt id chile\\[20.12.15, 20:10:12] Papa: Du bisch so suppppper\\[21.12.15, 12:22:44] Holzmano: Wer isch dihei?\\[21.12.15, 12:23:13] Vera: ich papa\\[21.12.15, 12:24:04] Holzmano: Chilli │ a: hch hinde nsee  e ds shon   19.02.17, 10:00:06] Hapa: Ha misch do gceeee e  [08.02.16, 10:00:06] Polzmano: Han ssch da     [18.02.17, 10:00:06] Pera: hch hase  04.02.17, 10:00:06] Holzmano: Hhanl   │ loss: 1.18288\n",
            "└───────INDEX──────────BOOK NAME────┴────────────────────────────────────────────────────────────────────────────────────────────TRAINING SEQUENCE─────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────PREDICTED SEQUENCE────────────────────────────────────────────────────────────────────────────────────────────┴────LOSS─────┘\n",
            "\n",
            "TRAINING STATS: batch 84/102 in epoch 8,     batch loss: 1.36333, batch accuracy: 0.56360\n",
            "\n",
            "Validating on chat_felix.txt                               \n",
            "\n",
            "VALIDATION STATS:                                  loss: 1.42988,       accuracy: 0.53430\n",
            "\n",
            "┌───────────────────────────────────Generating random text from learned state───────────────────────────────────┐\n",
            "aausche den Schoerten das isch denn das isch no de wern wenn de Stunder de Stuete de Sind werd de wa\n",
            "└───────────────────────────────────────────────End of generation───────────────────────────────────────────────┘\n",
            "\n",
            "0%                                        Training on next 50 batches                                        100%\n",
            "=================================================================================================================\n",
            "38000000 (epoch 9) chat_mama.txt │ ke...\\[08.06.14, 14:13:53] Holzmano: Au kuul :)\\[08.06.14, 14:13:56] Rafiq of the Many Baumann: Lukip?\\[08.06.14, 14:14:03] Holzmano: Yu\\[08.06.14, 14:14:44] Holzmano: Gang denn glaub nachene no schne │ e  . [27.06.16, 12:22:22] Holzmano: Hl doet s) [27.06.16, 12:27:22] Rafiq of the Many Baumann: Hoe    [27.06.18, 12:22:22] Holzmano: Hee 27.06.17, 12:29:29] Rolzmano: Hendede n deaub doche   soedchoil │ loss: 1.04412\n",
            "38000200 (epoch 9) chat_mama.txt │ man: \u0000Bild weggelassen\\\u0000[20.12.17, 22:54:35] Paula Guzman: \u0000Bild weggelassen\\[20.12.17, 22:58:16] Paula Guzman:  - min guet nacht kuss.\\[20.12.17, 22:58:38] Holzmano:  zwei zrugg!!\\[21.12.17, 08:00:55 │ an: \u0000Bild weggelassen [[17.11.17, 10:27:27] Haula Guzman: \u0000Bild weggelassen [17.11.17, 10:22:27] Haula Guzman: Wa dit maet docht dae  .[17.11.17, 10:22:22] Holzmano: Hauar suae   ![27.02.17, 10:32:29] │ loss: 0.80999\n",
            "38000400 (epoch 9) chat_mama.txt │ klassiker ond chatze us.\\[12.10.17, 17:43:33] Sadhbh Soziologie: De DJ froegt ab wenn daser morn dete waered?\\[12.10.17, 17:44:30] Toniba Sofa: ich ha gmeint 7ni\\[12.10.17, 17:44:43] Toniba Sofa: aber │ oanse    sdd shon   dn  [17.02.17, 12:43:46] Ctdhbh Soziologie: Iansa saeese dbedenn des   dirn de   sesr    [27.00.17, 12:53:56] Coniba Sofa:  ch hanaeain  d   [27.02.17, 12:23:22] Coniba Sofa:  ler  │ loss: 1.28889\n",
            "38000600 (epoch 9) chat_mama.txt │ st die letzte Moeglichkeit, brieflich abzustimmen\\\u0000[22.02.16, 18:41:39] Papa: \u0000Bild weggelassen\\[22.02.16, 18:45:12] Felix: Mmmmh! Chume abr spaat!:)\\[23.02.16, 10:14:21] Vera: ich hanen sechser ide m │ cedae saite  sarnee h e    diunbee h due e eg er [[27.06.18, 18:49:19] Papa: \u0000Bild weggelassen [27.06.16, 18:59:59] Pelix: He     Shane due dcaet    [27.06.16, 18:58:59] Hera: hch han   dohht   ss  sa │ loss: 1.40538\n",
            "└───────INDEX─────────BOOK NAME──┴────────────────────────────────────────────────────────────────────────────────────────────TRAINING SEQUENCE─────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────PREDICTED SEQUENCE────────────────────────────────────────────────────────────────────────────────────────────┴────LOSS─────┘\n",
            "\n",
            "TRAINING STATS: batch 32/102 in epoch 9,     batch loss: 1.36438, batch accuracy: 0.56463\n",
            "\n",
            "Validating on chat_felix.txt                               \n",
            "\n",
            "VALIDATION STATS:                                  loss: 1.42354,       accuracy: 0.53817\n",
            "\n",
            "0%                                        Training on next 50 batches                                        100%\n",
            "=================================================================================================================\n",
            "40000000 (epoch 9) chat_familie.txt │ :10:57] Rafiq of the Many Baumann: Isch guet gsi, s'Tessin?\\[10.07.14, 00:25:39] Holzmano: Jop sehr...bin aber wider dehei...\\[10.07.14, 00:26:51] Rafiq of the Many Baumann: Where else?\\[10.07.14, 10: │ 05:26] Hafiq of the Many Baumann: Wcch deet dee  dchanc    [28.06.17, 10:00:26] Holzmano: Ha  [chr .. in duer deeer de e  .. [28.06.17, 10:00:26] Rafiq of the Many Baumann: Ween  sssc  [28.06.17, 11:0 │ loss: 1.00473\n",
            "40000200 (epoch 9) chat_familie.txt │ easy kabutt, aber du hesch ja iwie 2h weniger pennt...\\Snebe dir schlafe isch mega schoen gsi...bitz warm! Aber da isch au eifach chlis sofa tschuld...da schwitzisch eifach easy fescht druff!\\\\Es isch │ i   denee   dber de setch da dseerd  denn    drrne .. [cederdee schoisf gsch dira dcho nddec .. in  des e Wber desssch du dsnach dhoi sgc aldechtet .. essihoeet g h dinach di   durch  deue e [1s gsch  │ loss: 1.88018\n",
            "40000400 (epoch 9) chat_familie.txt │ , 02:02:03] Fabienne Soziologie: Kennt oepper e noemi?\\[14.10.17, 02:02:26] Fabienne Soziologie: (Jahrgang 1996)\\[14.10.17, 02:03:49] Toniba Sofa: aeaeaehh\\[14.10.17, 02:03:50] Toniba Sofa: no\\[14.10. │  12:25:36] Cabienne Soziologie: Iaine depper dsse de  [14.02.17, 12:25:26] Cabienne Soziologie: Iaa a ehd d .00 [24.02.17, 12:22:26] Coniba Sofa:  ls      [28.02.17, 12:22:26] Coniba Sofa:  ee 01.02.1 │ loss: 1.08416\n",
            "40000600 (epoch 9) chat_familie.txt │ cules achoo?\\[29.03.16, 19:22:00] Papa: Wajdas? Hosetraegr?\\[29.03.16, 19:22:23] Papa: Per Post ijs noed xo\\[30.03.16, 19:51:15] Papa: Fahre jetz ap ide juel\\[30.03.16, 19:51:50] Felix: \\\u0000[01.04.16, 2 │ h e  gmh    [28.06.14, 18:07:26] Mapa: Wasa    Wae    ese  [28.06.14, 18:05:26] Hapa: Waitsaet as  goed ae [00.06.14, 18:05:46] Hapa: Waer  satz gm sse gaete 00.06.14, 18:05:26] Helix: W[[28.06.18, 10 │ loss: 1.47028\n",
            "└───────INDEX──────────BOOK NAME────┴────────────────────────────────────────────────────────────────────────────────────────────TRAINING SEQUENCE─────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────PREDICTED SEQUENCE────────────────────────────────────────────────────────────────────────────────────────────┴────LOSS─────┘\n",
            "\n",
            "TRAINING STATS: batch 82/102 in epoch 9,     batch loss: 1.31550, batch accuracy: 0.57512\n",
            "\n",
            "Validating on chat_felix.txt                               \n",
            "\n",
            "VALIDATION STATS:                                  loss: 1.40446,       accuracy: 0.54239\n",
            "Saved file: drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_models/rnn_train_1548172463-40000000\n",
            "\n",
            "0%                                        Training on next 50 batches                                        100%\n",
            "============================================="
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_GwWRVf9z4Cb",
        "colab_type": "code",
        "outputId": "7ccb6642-9d96-485d-c43b-6b811c11e67c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "# Generate a series of documents with different topn parameters\n",
        "sample_docs = 'drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/sample_docs'\n",
        "for i in [2,3,5,8,10,15,20,30,50,100,0]:\n",
        "  text_ = generate_sample(topn=i,author=model_name)\n",
        "  text_ = text_.replace('[','\\n[')\n",
        "  with open(sample_docs+'/sample_'+str(i)+'.txt','w+') as outp:\n",
        "    outp.write(text_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_models/final_rnn_train_chats-40800000\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_models/final_rnn_train_chats-40800000\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_models/final_rnn_train_chats-40800000\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_models/final_rnn_train_chats-40800000\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_models/final_rnn_train_chats-40800000\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_models/final_rnn_train_chats-40800000\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_models/final_rnn_train_chats-40800000\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_models/final_rnn_train_chats-40800000\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_models/final_rnn_train_chats-40800000\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_models/final_rnn_train_chats-40800000\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/NNandNLP/tensorflow-rnn-shakespeare-master/miniproject/chat_models/final_rnn_train_chats-40800000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o4U-8Gq83u0B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}